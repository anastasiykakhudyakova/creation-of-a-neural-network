#Переобучение (overfitting) - ситуация, когда на тренировочных данных нейросеть выдает отличные результаты,
#на случайных или тестовых данных результаты ухудшаются.
#Происходит при большом количестве итераций.

"""задание"""
#Увеличьте или уменьшите количество итераций в цикле обучения (например, до 10000 или до 100).
#Как это влияет на точность предсказания и ошибку?
#Когда нейросеть перестает улучшать предсказания?
import numpy as np
# Инициализация начальных весов нейронной сети
# weights[0] - коэффициент для роста, weights[1] - коэффициент для веса
weights = np.array([0.2, 0.3])

# Функция нейронной сети - выполняет линейное преобразование
# inp - входные данные (массив роста и веса), weights - весовые коэффициенты
# Возвращает скалярное произведение (линейную комбинацию входов и весов)
def neural_networks(inp, weights):
    return inp.dot(weights)

# Функция вычисления квадратичной ошибки
# true_prediction - целевое (правильное) значение
# prediction - предсказанное значение нейросетью
# Квадратичная ошибка используется для оценки точности предсказания
def get_error(true_prediction, prediction):
    return (true_prediction - prediction) ** 2

# Функция градиентного спуска для обучения нейронной сети
# inp - массив входных данных
# true_predictions - массив целевых значений
# weights - весовые коэффициенты
# learning_rate - скорость обучения (гиперпараметр)
# epochs - количество эпох обучения (итераций)
def gradient(inp, true_predictions, weights, learning_rate, epochs):
    # Цикл по количеству эпох обучения
    for i in range(epochs):
        error = 0  # Обнуление суммарной ошибки для текущей эпохи
        delta = np.zeros_like(weights)  # Создание нулевого массива для накопления градиентов
        #np.zeros_like() — это функция библиотеки NumPy, которая создает новый массив с той же формой и типом данных, что и входной массив, но заполненный нулями.
      
        # Проход по всем примерам обучающей выборки
        for j in range(len(inp)):
            current_inp = inp[j]  # Текущий входной пример (рост и вес)
            true_prediction = true_predictions[j]  # Целевое значение для текущего примера
            prediction = neural_networks(current_inp, weights)  # Предсказание нейросети
            error += get_error(true_prediction, prediction)  # Накопление суммарной ошибки
            
            # Вывод отладочной информации о предсказании и весах
            print(
                "Prediction: %.10f, True_prediction: %.10f, Weights: %s"
                % (prediction, true_prediction, weights)
            )
            
            # Вычисление градиента для текущего примера
            delta += (prediction - true_prediction) * current_inp * learning_rate
        
        # Обновление весов: вычитаем средний градиент по всем примерам
        weights -= delta / len(inp)
        
        # Вывод суммарной ошибки для текущей эпохи
        print("Errors: %.10f" % error)
        print("-------------------")  # Разделитель между эпохами
    
    # Возвращаем обученные веса после всех эпох
    return weights

# Вспомогательная функция для вычисления вероятности принадлежности к мужскому полу
# person_h - рост человека в см, person_w - вес человека в кг
def calc_prob(person_h, person_w):
    return neural_networks(np.array([person_h, person_w]), weights)

# Обучающая выборка: массив пар [рост, вес]
inp = np.array(
    [
        [150, 40],  # Женщина
        [140, 35],  # Женщина
        [155, 45],  # Женщина
        [185, 95],  # Мужчина
        [145, 40],  # Женщина
        [195, 100], # Мужчина
        [180, 95],  # Мужчина
        [170, 80],  # Мужчина
        [160, 90],  # Мужчина (аномалия: средний рост, большой вес)
    ]
)

# Целевые значения: 0 = женщина, 100 = мужчина
true_predictions = np.array([0, 0, 0, 100, 0, 100, 100, 100, 100])

# Скорость обучения
learning_rate = 0.00001

# Эксперимент с разным количеством эпох обучения:
# epochs = 100 # ошибка больше
#  при 100 эпохах сеть не успевает полностью обучиться,
# веса не достигают оптимальных значений, ошибка остается высокой

# epochs = 1000  # ошибка меньше
# 1000 эпох может быть достаточно для хорошего обучения,
# сеть достигает приемлемой точности, ошибка уменьшается

# epochs = 10**5  # ошибка очень большая
# здесь может быть 2 варианта:
# 1. Слишком много эпох для данной скорости обучения - веса "улетают" в бесконечность
# 2. Или наоборот - сеть переобучается на аномалиях в данных

# обучение
weights = gradient(inp, true_predictions, weights, learning_rate, epochs)

# Тестирование обученной модели
print(calc_prob(150, 45))  # Ожидается значение ближе к 0 (женщина)
print(calc_prob(170, 85))  # Ожидается значение ближе к 100 (мужчина)

