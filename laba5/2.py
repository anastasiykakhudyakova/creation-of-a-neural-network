#Измените скорость обучения (learning_rate). 
#Попробуйте более высокие или более низкие значения (например, 0.000001 или 0.0001). 
#Как это влияет на скорость сходимости и точность предсказания?
# learning_rate = 0.000001 # ошибка становится больше так как нужно больше эпох для обучения
# Причина: слишком маленькая скорость обучения приводит к очень медленной сходимости.
# Для достижения того же результата потребовалось бы значительно больше эпох.

# learning_rate = 0.0001 # ошибки при рассчетах
# Причина: слишком большая скорость обучения может вызывать:
# 1. Расходимость (веса "скачут" и не сходятся к оптимальным значениям)
# 2. Перескакивание через минимум ошибки
# 3. Нестабильность обучения
''' задание'''
import numpy as np
# Инициализация начальных весов нейронной сети
# weights[0] - коэффициент для роста, weights[1] - коэффициент для веса
weights = np.array([0.2, 0.3])

# Функция нейронной сети - выполняет линейное преобразование
# inp - входные данные (массив роста и веса), weights - весовые коэффициенты
# Возвращает скалярное произведение (линейную комбинацию входов и весов)
def neural_networks(inp, weights):
    return inp.dot(weights)

# Функция вычисления квадратичной ошибки
# true_prediction - целевое (правильное) значение
# prediction - предсказанное значение нейросетью
# Квадратичная ошибка используется для оценки точности предсказания
def get_error(true_prediction, prediction):
    return (true_prediction - prediction) ** 2

# Функция градиентного спуска для обучения нейронной сети
# inp - массив входных данных
# true_predictions - массив целевых значений
# weights - весовые коэффициенты
# learning_rate - скорость обучения (гиперпараметр)
# epochs - количество эпох обучения
def gradient(inp, true_predictions, weights, learning_rate, epochs):
    # Цикл по количеству эпох обучения
    for i in range(epochs):
        error = 0  # Обнуление суммарной ошибки для текущей эпохи
        delta = np.zeros_like(weights)  # Создание нулевого массива для накопления градиентов
        
        # Проход по всем примерам обучающей выборки (стохастический градиентный спуск)
        for j in range(len(inp)):
            current_inp = inp[j]  # Текущий входной пример (рост и вес)
            true_prediction = true_predictions[j]  # Целевое значение для текущего примера
            prediction = neural_networks(current_inp, weights)  # Предсказание нейросети
            error += get_error(true_prediction, prediction)  # Накопление суммарной ошибки
            
            # Вывод отладочной информации о предсказании и весах
            print("Prediction: %.10f, True_prediction: %.10f, Weights: %s" % (prediction, true_prediction, weights))
            
            # Вычисление градиента для текущего примера по правилу цепочки:
            # градиент = (предсказание - цель) * входные данные * скорость_обучения
            delta += (prediction - true_prediction) * current_inp * learning_rate
        
        # Обновление весов: вычитаем средний градиент по всем примерам
        # Деление на len(inp) усредняет градиент по всей выборке
        weights -= delta / len(inp)
        
        # Вывод суммарной ошибки для текущей эпохи
        print("Errors: %.10f" % error)
        print("-------------------")  # Разделитель между эпохами
    
    # Возвращаем обученные веса после всех эпох
    return weights

# Вспомогательная функция для вычисления вероятности принадлежности к мужскому полу
# person_h - рост человека в см, person_w - вес человека в кг
# Возвращает значение нейросети, где значения ближе к 100 = мужчина, ближе к 0 = женщина
def calc_prob(person_h, person_w):
    return neural_networks(np.array([person_h, person_w]), weights)

# Обучающая выборка: массив пар [рост, вес]
inp = np.array([
    [150, 40],  # Женщина (маленький рост и вес)
    [140, 35],  # Женщина (очень маленький рост и вес)
    [155, 45],  # Женщина (средний рост, небольшой вес)
    [185, 95],  # Мужчина (высокий рост, большой вес)
    [145, 40],  # Женщина (низкий рост, небольшой вес)
    [195, 100], # Мужчина (очень высокий рост, очень большой вес)
    [180, 95],  # Мужчина (высокий рост, большой вес)
    [170, 80],  # Мужчина (средний рост, средний вес)
    [160, 90],  # Мужчина (средний рост, большой вес) - аномалия в данных
])

# Целевые значения: 0 = женщина, 100 = мужчина
true_predictions = np.array([0, 0, 0, 100, 0, 100, 100, 100, 100])

# Текущая скорость обучения (оптимальная для этого набора данных)
learning_rate = 0.00001

# Количество эпох обучения (полных проходов по всем данным)
epochs = 500

# Обучение нейронной сети: вызываем функцию градиентного спуска
# На выходе получаем обученные веса
weights = gradient(inp, true_predictions, weights, learning_rate, epochs)

# Тестирование обученной модели:
print(calc_prob(150, 45))  # Ожидается значение ближе к 0 (женщина)
print(calc_prob(170, 85))  # Ожидается значение ближе к 100 (мужчина)
