"""Скрытые слои и функция ReLU"""
#Предыдущие работы описывали создание простых нейронных сетей. 
#Особенность подобных сетей заключается в наличии определенной корреляцией между входными и выходными значениями. 
#Например, в решении задачи об определении пола человека по росту и весу, глядя на исходные данные мы сразу предположим, какой пол определит нейронная сеть.
#Однако, очень часто не бывает такой ярко выраженной корреляции.Например, найти на фото галза человека. 
#Здесь корреляция между входными данными и выходными сложно просматривается. 
#Конечно мы можем обучить нейронную сеть на 1000 изображений, но стоит только добавить 1001 новую и тутже нейросеть неправильно определит расположение глаз.
#Поэтому для создания нейросетей, решающих сложные задача необходимо добавлять дополнительный слой, в котором коррелирующие данные все таки появятся. 
"""Т.е. скрытый слой нужен для того, чтобы создать некую корреляцию между входом и выходом."""
#При создании скрытого слоя нужно учитывать тот факт, что при тривиальном создании скрытого слоя, используя простую математику мы легко преобразуем сложную нейросеть в простую.
#Т.е. обычное задание коэффициентов не приводит к корреляции на уровне скрытого слоя. Для того, чтобы получить правильный скрытый слой используют функцию активации.
#Функции активации вводят нелинейность. Т.е. скрытый слой рассчитывается уже не обычным перемножением входного значения на вес выходного, но и дополнительно умножается на спец функцию активации.
#Одна из самых популярных подобных функций это функция ReLU – Rectifiet Linear Unit
"""ReLU (Rectified Linear Unit) - это нелинейная функция активации, которая широко используется в глубоком обучении. Она преобразует входное значение в значение от 0 до положительной бесконечности.
Если входное значение меньше или равно нулю, то ReLU выдает ноль, в противном случае - входное значение.
Математически ReLU определяется следующим образом:  ReLU(x) = max(0, x) где max - функция, возвращающая максимальное значение из двух."""


""" задание"""
#Напишите по памяти код из урока "Скрытые слои и функция ReLu".
import numpy as np
# Функция для расчета ошибки между предсказаниями и истинными значениями
def get_error(true_prediction, prediction):
    # Возвращает квадратный корень из среднеквадратичной ошибки (RMSE)
    return np.sqrt(np.mean((true_prediction - prediction) ** 2))

# Определение функции активации ReLU (Rectified Linear Unit)
def relu(x):
    # Если x > 0, возвращает x, иначе возвращает 0
    return (x > 0) * x

# Входные данные: массив из 4 примеров, каждый содержит 2 признака
inp = np.array([[15, 10], [15, 15], [15, 20], [25, 10]])

# Истинные (целевые) значения для предсказания (транспонированный массив)
true_prediction = np.array([[10, 20, 15, 20]]).T

# Количество нейронов в скрытом слое
layer_hid_size = 3

# Размер входного слоя: количество признаков в каждом примере
layer_in_size = len(inp[0])

# Размер выходного слоя: количество выходных значений
layer_out_size = len(true_prediction[0])

# Инициализация весов для скрытого слоя случайными значениями от -1 до 1
weights_hid = 2 * np.random.random((layer_in_size, layer_hid_size)) - 1

# Инициализация весов для выходного слоя случайными значениями от -1 до 1
weights_out = 2 * np.random.random((layer_hid_size, layer_out_size)) - 1

# Вывод матрицы весов скрытого слоя
print(weights_hid)

# Вывод матрицы весов выходного слоя
print(weights_out)

# Прямое распространение через скрытый слой:
# 1. Умножение первого входного примера на веса скрытого слоя
# 2. Применение функции активации ReLU к результату
prediction_hid = relu(np.dot(inp[0], weights_hid))

# Вывод активаций скрытого слоя после применения ReLU
print(prediction_hid)

# Прямое распространение через выходной слой:
# Умножение активаций скрытого слоя на веса выходного слоя
prediction = prediction_hid.dot(weights_out)
print(prediction)

#Количество слоев: 3 (1 входной, 1 скрытый, 1 выходной)
#Входные нейроны: 2 (2 признака на каждый пример: [15, 10], [15, 15], [15, 20], [25, 10]) 
#Скрытый слой: 1 слой с 3 нейронами(layer_hid_size = 3)
#Выходные нейроны: 1


""" задание"""
#Увеличьте количество нейронов в скрытом слое.
import numpy as np

# Функция вычисления ошибки между истинными значениями и предсказаниями
def get_error(true_prediction, prediction):
    # Возвращает RMSE (среднеквадратичную ошибку) - квадратный корень из среднего квадрата разностей
    return np.sqrt(np.mean((true_prediction - prediction) ** 2))

# Функция активации ReLU (Rectified Linear Unit)
def relu(x):
    # Реализация ReLU: возвращает x если x>0, иначе 0
    # (x > 0) создает булев массив, который при умножении на x обнуляет отрицательные значения
    return (x > 0) * x

# Массив входных данных: 4 примера, каждый с 2 признаками
inp = np.array([[15, 10], [15, 15], [15, 20], [25, 10]])

# Массив истинных значений (таргетов) для обучения
# .T - транспонирование, чтобы получить вектор-столбец
true_prediction = np.array([[10, 20, 15, 20]]).T

# Количество нейронов в скрытом слое - УВЕЛИЧЕНО ДО 5 (было 3)
layer_hid_size = 5

# Определение размера входного слоя = количество признаков в каждом примере
layer_in_size = len(inp[0])

# Определение размера выходного слоя = количество выходных значений
layer_out_size = len(true_prediction[0])

# Инициализация весов скрытого слоя случайными значениями
# Матрица размером (2, 5): 2 входа × 5 нейронов в скрытом слое
# np.random.random создает случайные числа от 0 до 1
# 2 * ... - 1 преобразует диапазон в [-1, 1]
weights_hid = 2 * np.random.random((layer_in_size, layer_hid_size)) - 1

# Инициализация весов выходного слоя случайными значениями
# Матрица размером (5, 1): 5 нейронов скрытого слоя × 1 выход
weights_out = 2 * np.random.random((layer_hid_size, layer_out_size)) - 1

# Прямое распространение (forward propagation) на скрытом слое:
# 1. np.dot(inp[0], weights_hid) - линейное преобразование входных данных
# 2. relu(...) - применение функции активации ReLU
# inp[0] - берется только первый пример из обучающей выборки [15, 10]
prediction_hid = relu(np.dot(inp[0], weights_hid))

# Вывод активаций скрытого слоя (5 значений, по одному на каждый нейрон)
print(prediction_hid)

# Прямое распространение на выходном слое:
# Умножение активаций скрытого слоя на веса выходного слоя
prediction = prediction_hid.dot(weights_out)
print(prediction)
#Количество слоев: 3 (1 входной, 1 скрытый, 1 выходной)
#Входные нейроны: 2
#Скрытый слой: 1 слой с 5 нейронами (layer_hid_size = 5, УВЕЛИЧЕНО с 3 до 5)
#Выходные нейроны: 1


""" задание"""
#Добавьте ещё один скрытый слой.
#Его значения надо также пропустить через функцию ReLU.
#Проанализируйте результат.
import numpy as np

# Функция для вычисления ошибки между истинными и предсказанными значениями
def get_error(true_prediction, prediction):
    # Вычисление среднеквадратичной ошибки (RMSE)
    # 1. Вычисление разности между истинными и предсказанными значениями
    # 2. Возведение в квадрат каждой разности
    # 3. Вычисление среднего значения квадратов ошибок
    # 4. Извлечение квадратного корня
    return np.sqrt(np.mean((true_prediction - prediction) ** 2))

# Функция активации ReLU (Rectified Linear Unit)
def relu(x):
    # Если значение больше 0 - возвращает значение, иначе 0
    # Реализация через булеву маску: (x > 0) создает маску True/False
    # При умножении маски на x отрицательные значения становятся 0
    return (x > 0) * x

# Массив входных данных (признаки)
# 4 примера, каждый с 2 признаками (например, рост и вес)
inp = np.array([[15, 10], [15, 15], [15, 20], [25, 10]])

# Истинные значения (таргеты) для обучения
# Транспонирование .T для преобразования в вектор-столбец
true_prediction = np.array([[10, 20, 15, 20]]).T

# Определение размеров слоев нейронной сети
# Размер входного слоя = количество признаков (2)
layer_in_size = inp.shape[1]  # Аналогично len(inp[0]), но более безопасно

# Размер первого скрытого слоя - 10 нейронов
layer_hid1_size = 10

# Размер второго скрытого слоя - 7 нейронов
layer_hid2_size = 7

# Размер выходного слоя = количество выходных значений (1)
layer_out_size = true_prediction.shape[1]

# Инициализация весов первого скрытого слоя
# Матрица размером (2, 10): 2 входа × 10 нейронов
# Случайные значения в диапазоне [-1, 1]
weights_hid1 = 2 * np.random.random((layer_in_size, layer_hid1_size)) - 1

# Инициализация весов второго скрытого слоя
# Матрица размером (10, 7): выход первого слоя (10) × вход второго (7)
weights_hid2 = 2 * np.random.random((layer_hid1_size, layer_hid2_size)) - 1

# Инициализация весов выходного слоя
# Матрица размером (7, 1): выход второго слоя (7) × выход сети (1)
weights_out = 2 * np.random.random((layer_hid2_size, layer_out_size)) - 1

# Прямое распространение через первый скрытый слой
# 1. Линейное преобразование: inp × weights_hid1
# 2. Применение функции активации ReLU
# Обрабатываются ВСЕ примеры (inp), а не только первый (inp[0])
prediction_hid1 = relu(np.dot(inp, weights_hid1))

# Прямое распространение через второй скрытый слой
# 1. Линейное преобразование: prediction_hid1 × weights_hid2
# 2. Применение функции активации ReLU
prediction_hid2 = relu(np.dot(prediction_hid1, weights_hid2))

# Прямое распространение через выходной слой
# Линейное преобразование: prediction_hid2 × weights_out
prediction = np.dot(prediction_hid2, weights_out)

# Вывод промежуточных результатов для анализа

print("layer number 1")  # Метка для первого скрытого слоя
print(prediction_hid1)  # Вывод активаций первого скрытого слоя (4×10 матрица)

print("layer number 2")  # Метка для второго скрытого слоя
print(prediction_hid2)  # Вывод активаций второго скрытого слоя (4×7 матрица)

print("output")  # Метка для выходного слоя
print(prediction)  # Вывод финальных предсказаний (4×1 матрица)

# Вычисление и вывод ошибки
error = get_error(true_prediction, prediction)  # Вычисление RMSE
print("RMSE:", error)  # Вывод значения ошибки
#Количество слоев: 4 (1 входной, 2 скрытых, 1 выходной)
#Входные нейроны: 2
#Скрытые слои: 2 слоя - Первый скрытый слой: 10 нейронов (layer_hid1_size = 10) - Второй скрытый слой: 7 нейронов (layer_hid2_size = 7)
#Выходные нейроны: 1
