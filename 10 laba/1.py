"""Задания для самостоятельной работы
Функция активации sigmoid
1.Попробуйте изменить функцию активации (например, на relu) и сравните результаты. Какая функция активации лучше подходит для этой задачи?
2.Попробуйте изменить количество нейронов в скрытом слое (hidden_size). Как это влияет на сходимость обучения и точность предсказания?
Функция активации tanh
1.Каково влияние выбора tanh на обучение и точность сети? Сравните результаты с использованием sigmoid, tanh, relu. Какая функция активации подходит лучше всего для этой задачи?"""

"""Функции активации sigmoid """
#сигмовидная функция принимает любое реальное значение и сжимает его в диапазоне от 0 до 1.

"""Наиболее распространенная сигмовидная функция называется логистической функцией. 
Данная функция нашла применение  в задачах двоичной классификации, и имеет красивую S-образную кривую
Мы можем выразить это как Y = 1 / (1 + e^(-z)), где Y — выходной сигнал, e — число Эйлера (2,71828), а z — входные данные. 
По сути, сигмовидная функция превращает наше входное значение в вероятность!

Выходной диапазон сигмовидной функции от 0 до 1 идеально подходит для определения вероятности того, что входные данные принадлежат одной из групп (задачи бинарной классификаци). 
Если выходной сигнал превышает 0,5, он принадлежит к одной группе; если он падает ниже, он классифицируется как другой.

роль сигмоиды как функции активации
В контексте нейронной сети сигмовидная функция широко используется в качестве функции активации, определяющей выходные данные узла с учетом входных данных или набора входных данных.
Это помогает внести нелинейность в выходной сигнал нейрона."""

#Преимущества сигмовидной функции в машинном обучении
#Сигмовидная функция отличается своей простотой и дифференцируемостью. Её легко вычислить, а результаты можно использовать для создания распределения вероятностей для задач принятия решений.
#Недостатки и альтернативы сигмоиды
#Проблема исчезновения градиентов для очень больших положительных и отрицательных входных данных.

#Рассмотренная ранее функция ReLu (полулинейная функция активации) не страдает от этой проблемы.
#Каковы некоторые распространенные типы сигмовидных функций?Если назвать несколько, у нас есть
#логистическая сигмовидная функция, функция гиперболического тангенса, функция арктангенса, функция Гуддермана, функция ошибок, функция плавного шага и обобщенная логистическая функция.

# что функция весьма быстро доходит при входных параметрах до значений близких к 1. Т.е. разницы между входным значением 4 и 504 практически нет. 
#Это позволяет нам сглаживать частично некорректные значения – такие как выбросы. 
#Что такое выброс?То какая-то ошибка или, иначе, выброс, который лучше не учитывать.

import numpy as np 
# функции активации и их производные — три варианта для сравнения
def sigmoid(x):
    """берём число (или массив чисел) и преобразуем его в значение от 0 до 1: большие положительные → ближе к 1, большие отрицательные → ближе к 0"""
    return 1 / (1 + np.exp(-x))  # стандартная формула: 1 / (1 + e^(-x)) сигмоиды #реализуем чтото вроде "исключающего или" XOR 

def sigmoid_deriv(x):
    """вычисляем производную сигмоиды — она нужна для обучения; работает только если x — уже результат sigmoid (т.е. лежит в [0,1])"""
    return x * (1 - x)  # упрощённая формула производной: σ'(x) = σ(x)·(1−σ(x))

"""Функция активации tanh""" 
"""При обучении многослойных нейронных сетей наиболее часто используются сигмоидальные активационные функции, названные так за их характерную S-образную форму.
Примерами таких функций являются гиперболический тангенс и логистическая функция.

Гиперболический тангенс очень похож на сигмоиду. И действительно, это скорректированная сигмоидная функция.
Поэтому такая функция имеет те же характеристики, что и у сигмоиды, рассмотренной ранее.
Её природа нелинейна, она хорошо подходит для комбинации слоёв, а диапазон значений функции -(-1, 1). 
Поэтому нет смысла беспокоиться, что активационная функция перегрузится от больших значений.
Однако стоит отметить, что градиент тангенциальной функции больше, чем у сигмоиды (производная круче).
Решение о том, выбрать ли сигмоиду или тангенс, зависит от ваших требований к амплитуде градиента.
Также как и сигмоиде, гиперболическому тангенсу свойственная проблема исчезновения градиента.
Тангенс также является очень популярной и используемой активационной функцией.
При сравнении сигмоиды и гиперболического тангенса мы видим, что в отличии от сигмоиды, значения в которой меняются от 0 до 1, значения гиперболического тангенса меняются от -1 до 1. 
В некоторых задачах такой результат наиболее интересен.
Особенно если средние значения данных ближе к нулю, потому что в этом случае гиперболический тангенс даст нам более качественное и устойчивое обучение нейросети. 
Также нужно заметить, что к своим пороговым значениям гиперболический тангенс движется быстрее чем сигмоида."""
def tanh(x):
    """берём число и сжимаем его в диапазон от -1 до +1 — похоже на сигмоиду, но симметрично относительно нуля"""
    return np.tanh(x)  # встроенная функция гиперболического тангенса
#Запишем формулу гиперболического тангенса:
#Найдем производную данной функции
def tanh_deriv(x):
    """вычисляем производную tanh — она показывает, насколько быстро меняется выход при изменении входа; x должен быть результатом tanh"""
    return 1 - x ** 2  # производная: tanh'(x) = 1 − tanh²(x)

def relu(x):
    """пропускаем только положительные значения, все отрицательные заменяем на 0 — простая и быстрая функция"""
    return np.maximum(0, x)  # np.maximum возвращает поэлементно большее из 0 и x

def relu_deriv(x):
    """возвращаем 1 там, где x > 0 (сигнал прошёл), и 0 — где x <= 0 (сигнал заблокирован)"""
    return (x > 0).astype(float)  # (x > 0) даёт логические значения, .astype(float) превращает их в 1.0 и 0.0

# -----------------------------
# данные для задачи xor — простая, но требует нелинейного решения
# -----------------------------

x_train = np.array([[0, 0],
                    [0, 1],
                    [1, 0],
                    [1, 1]])  # задаём все возможные пары входов: 0 и 1 #входные данные

y_train = np.array([[0],
                    [1],
                    [1],
                    [0]])  # правильные ответы: xor даёт 1 только если входы разные #ожидаемый прогноз

# -----------------------------
# функция обучения сети — принимает настройки и возвращает результат
# -----------------------------

def train_network(
    activation_name="sigmoid",   # название функции активации для скрытого слоя
    hidden_size=4,              # сколько нейронов в промежуточном слое
    learning_rate=0.1,          # насколько сильно меняем веса за один шаг обучения
    epochs=10000,               # сколько раз прогоняем обучение по всем данным  #простые вычисления и обучение построим на всем наборе входных данных
    seed=1                      # номер для генератора случайных чисел — чтобы результаты повторялись
):
  
    """
    обучаем двухслойную сеть на задаче xor.
    в зависимости от activation_name выбираем разные функции активации и настройки выхода.
    """

    # определяем, какие функции активации и их производные использовать для скрытого слоя
    if activation_name == "sigmoid":
        act_hid = sigmoid        # функция активации в скрытом слое
        deriv_hid = sigmoid_deriv  # её производная
        act_out = sigmoid        # на выходе тоже сигмоида — чтобы результат был в [0,1]
        deriv_out = sigmoid_deriv  # и её производная
    elif activation_name == "tanh":
        act_hid = tanh
        deriv_hid = tanh_deriv
        act_out = tanh           # на выходе tanh — тогда ответы должны быть в [-1,1]
        deriv_out = tanh_deriv
    elif activation_name == "relu":
        act_hid = relu
        deriv_hid = relu_deriv
        act_out = lambda x: x    # на выходе линейная функция (просто возвращаем x без изменений)
        deriv_out = lambda x: np.ones_like(x)  # производная линейной функции — 1 везде
    else:
        raise ValueError("неизвестная функция активации")  # если ввели что-то не то — сообщаем об ошибке

    # копируем входные данные, чтобы не повредить оригиналы
    x = x_train.copy()

    # подготавливаем правильные ответы в зависимости от выходной активации
    if activation_name == "tanh":
        # tanh выдаёт значения от -1 до +1, а у нас ответы 0 и 1 → конвертируем: 0 → -1, 1 → +1
        y = y_train.copy() * 2 - 1  # умножаем на 2: [0,1] → [0,2], вычитаем 1: →
[-1, +1]
    else:
        y = y_train.copy()  # для sigmoid и relu оставляем ответы как есть: [0,1]

    # определяем размеры: сколько входов и выходов
    input_size = x.shape[1]     # число столбцов в x — у нас 2 входа
    output_size = y.shape[1]    # число столбцов в y — у нас 1 выход

    # фиксируем генератор случайных чисел, чтобы каждый запуск давал одинаковые начальные веса
    np.random.seed(seed)

    # инициализируем веса случайными числами от 0 до 1 (достаточно для простой задачи)
    weight_hid = np.random.uniform(size=(input_size, hidden_size))  # веса от входов к скрытому слою
    weight_out = np.random.uniform(size=(hidden_size, output_size))  # веса от скрытого слоя к выходу

    # создаём список для сохранения ошибок каждые 1000 эпох (чтобы потом посмотреть, как обучение шло)
    error_history = []

    # начинаем цикл обучения: повторяем epochs раз
    for epoch in range(epochs):

        # прямой проход: сначала считаем, что получается на входе скрытого слоя
        layer_hid_input = np.dot(x, weight_hid)  # умножаем входы на веса — получаем "сигналы" для скрытого слоя
      # print(layer_hid)
      # exit(0)
        layer_hid = act_hid(layer_hid_input)      # применяем функцию активации — получаем выход скрытого слоя

        # затем считаем выход сети
        layer_out_input = np.dot(layer_hid, weight_out)  # сигналы от скрытого слоя к выходу
        layer_out = act_out(layer_out_input)              # применяем выходную активацию

        # считаем ошибку: среднее квадратов разницы между тем, что получили, и тем, что хотели
        error = np.mean((layer_out - y) ** 2)
# print(error)
# exit(0)
        # обратный проход: вычисляем, как подправить веса, чтобы уменьшить ошибку
#Рассчитаем дельты изменения весовых коэффициентов

        # сначала — поправка для выходного слоя:
        # разница между предсказанием и правдой × чувствительность выходной активации
        out_delta = (layer_out - y) * deriv_out(layer_out)

        # затем — поправка для скрытого слоя:
        # распространяем ошибку назад через выходные веса, и умножаем на чувствительность скрытой активации
        hid_delta = out_delta.dot(weight_out.T) * deriv_hid(layer_hid)

        # обновляем веса: вычитаем шаг × градиент (градиент — это направление роста ошибки, поэтому минус)
        weight_out -= learning_rate * layer_hid.T.dot(out_delta)  # обновление выходных весов
        weight_hid -= learning_rate * x.T.dot(hid_delta)          # обновление весов на входе

        # каждые 1000 эпох сохраняем текущую ошибку для анализа
        if epoch % 1000 == 0:
            error_history.append(error)

    # после обучения возвращаем все нужные данные: веса, историю ошибок, и функции активации (чтобы потом делать предсказания)
    return weight_hid, weight_out, error_history, act_hid, act_out

# -----------------------------
# функция проверки — делает предсказание на всех 4 вариантах xor
# -----------------------------

def predict_all(weight_hid, weight_out, act_hid, act_out):
    """проверяем, как сеть работает на всех возможных входах, и возвращаем таблицу результатов"""
    inputs = np.array([[0,0], [0,1], [1,0], [1,1]])  # все 4 входа
    expected = np.array([0, 1, 1, 0])                 # ожидаемые ответы

    results = []  # сюда будем складывать результаты по каждому входу

    # перебираем все входы по одному
    for i in range(len(inputs)):
        x = inputs[i:i+1]  # берём одну строку, оставляя размерность [1,2]

        # прямой проход: как при обучении
        hid = act_hid(np.dot(x, weight_hid))      # выход скрытого слоя
        out_raw = np.dot(hid, weight_out)         # линейный выход
        out = act_out(out_raw)                    # с активацией

        # для tanh: выход в [-1,1], а нам нужно [0,1] — конвертируем обратно
        if act_out == tanh:
            pred_val = (out[0,0] + 1) / 2  # -1 → 0, +1 → 1
        else:
            pred_val = out[0,0]  # для sigmoid и relu — уже в [0,1]

        # округляем до ближайшего целого: >0.5 → 1, иначе → 0
        pred_rounded = 1 if pred_val > 0.5 else 0

        # сохраняем: вход, точное значение, округлённое предсказание, ожидаемый ответ
        results.append((inputs[i], pred_val, pr
ed_rounded, expected[i]))

    return results  # возвращаем список кортежей

# -----------------------------
# основной запуск — сравниваем три функции активации при разных размерах скрытого слоя
# -----------------------------

def main():
    print("сравнение функций активации на задаче xor\n")
    print("="*70)

    # задаём параметры для экспериментов
    hidden_sizes = [2, 4, 8]     # пробуем три разных размера скрытого слоя
    activations = ["sigmoid", "tanh", "relu"]  # три функции активации
    lr = 0.1                     # фиксируем скорость обучения
    epochs = 20000               # достаточно для сходимости на xor

    all_results = []  # сюда сохраним все результаты для итоговой таблицы

    # перебираем все комбинации: размер слоя × функция активации
    for hs in hidden_sizes:
        print(f"\n→ размер скрытого слоя: {hs}")
        for act in activations:
            print(f"\n  {act}:")

            try:
                # запускаем обучение с текущими настройками
                w_hid, w_out, errors, act_hid, act_out = train_network(
                    activation_name=act,
                    hidden_size=hs,
                    learning_rate=lr,
                    epochs=epochs,
                    seed=42  # фиксируем seed для повторяемости
                )

                # берём последнюю ошибку из истории
                final_error = errors[-1]

                # проверяем точность на всех данных
                preds = predict_all(w_hid, w_out, act_hid, act_out)
                correct = sum(1 for _, _, pred, exp in preds if pred == exp)  # считаем, сколько угадали
                accuracy = correct / 4 * 100  # переводим в проценты

                # сохраняем результат в общий список
                all_results.append((act, hs, final_error, accuracy))

                # выводим детали
                print(f"    финальная ошибка: {final_error:.6f}")
                print(f"    точность: {accuracy:.1f}%")
                print("    вход → предсказание → ожидание")
                for inp, val, pred, exp in preds:
                    print(f"      {list(inp)} → {val:.4f} → {pred} (ожидалось {exp})")

            except Exception as e:
                # если вдруг обучение сломалось — сообщаем, но не прерываем программу
                print(f"    ошибка при обучении: {e}")
                all_results.append((act, hs, float('inf'), 0.0))

    # -----------------------------
    # итоговая таблица — сводка по всем экспериментам
    # -----------------------------
    print("\n" + "="*70)
    print("итоговое сравнение")
    print("="*70)
    print(f"{'активация':<10} | {'скрытый':<8} | {'ошибка':<12} | {'точность':<9}")
    print("-"*70)
    for act, hs, err, acc in all_results:
        # если была ошибка — пишем "ошибка", иначе — число
        err_str = f"{err:.6f}" if err != float('inf') else "ошибка"
        print(f"{act:<10} | {hs:<8} | {err_str:<12} | {acc:<9.1f}%")

# запускаем функцию main, только если этот файл запущен напрямую (а не импортирован)
if name == "__main__":
    main()
  # sigmoid: работает стабильно, но обучение идёт медленно; при маленьком hidden_size (2) иногда застревает в плохом решении.
  # tanh: часто сходится быстрее, чем sigmoid, потому что её выход симметричен относительно нуля — это помогает при обучении.
  # relu: самая быстрая, но при hidden_size=2 может не сходиться (если все нейроны 'выключатся' в начале). при hidden_size>=4 — отлично.
  #оптимальный выбор для xor: hidden_size=4 + tanh или relu. sigmoid требует больше эпох или меньшего learning_rate.
  # если обучение не сходится — попробуйте: увеличить hidden_size, уменьшить learning_rate, или изменить seed (начальные веса).


