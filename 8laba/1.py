"""Распознавание рукописных цифр"""

#Это классическая задача. Требуется разработать программную реализацию алгоритма для распознавания рукописных цифр.
#Реализация данного алгоритма должна происходить с помощью построения нейронной сети.
#Для обучения и тестирования предоставлена база данных MNIST, из которой будет взято 1000 образов для обучения и 10 000 образов для тестирования.
#Каждый образ представлен картинкой 28х28 пикселей с 256 градациями серого цвета.

""" MNIST — объёмная база данных образцов рукописного написания цифр.
База данных является стандартом, предложенным Национальным институтом стандартов и технологий США 
с целью калибрации и сопоставления методов распознавания изображений с помощью машинного обучения в первую очередь на основе нейронных сетей.
Данные состоят из заранее подготовленных примеров изображений, на основе которых проводится обучение и тестирование систем.
База данных MNIST содержит 60 000 изображений для обучения и 10 000 изображений для тестирования."""

"""MNIST загрузка - используется keras.datasets.mnist
One-Hot Encoding - преобразование цифр в векторы
ReLU активация - простая и эффективная функция
Прямое распространение - умножение матриц через np.dot()
Обратное распространение - расчет ошибок и обновление весов
Градиентный спуск - обучение по одному примеру за раз"""

#Для того, чтобы получить доступ к изображениям нам нужно обратиться к пакету keras (который тербуется дополнительно установить) и скачать mnist (from keras.src.datasets import mnist)
#нам потребуются скрытые слои. Поэтому будем использовать функцию активации ReLU производную от этой функции для вычисления дельты обратного распространения
"""задание
Напишите по памяти код из урока "Распознавание рукописных цифр".
Примечание: да, это сложновато, но нужно.
Необязательно всё делать точь-в-точь как было предложено
"""
import numpy as np
from keras.datasets import mnist

# Функция активации ReLU и ее производная
def relu(x):
    return np.maximum(0, x)

def relu_deriv(x):
    return x > 0

#Теперь нам нужно получить набор картинок. Точнее набор чисел. Поскольку каждая картинка это не что иное как матрица где 28 столбцов и 28 строк.
#В пересечении хранится цвет пикселя, определяемого оттенком серого: 0 – это черный цвет, 255 – это белый.
#Задаем переменные для тренировки и для будущего тестирования.)
# Загрузка данных MNIST
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Параметры
train_size = 1000# картинки для обучения
test_size = 10000#картинки для проверки работы НС
input_size = 28 * 28  # 784 пикселя размер картинок
hidden_size = 50 #Определяем размер матрицы весовых коэффициентов. Для этого нужно знать сколько нейронов будет на скрытом слое. 
#Данное количество подбирается индивидуально под задачу. Возьмем, например, 50 нейронов.
output_size = 10  # 10 цифр (0-9) #количество цифр, характеризует количество выходных нейронов со знаечнием в них в интервале от 0 до 1. Т.е.
#чем ближе к 1 тем более точный прогноз нейрона. Например, если у нас на 0 нейроне 1, то это скорее всего цифра 0. Если на первом, то это либо 1, либо 2 завистт от того как мы все распределим

# Подготовка тренировочных данных
train_images = x_train[:train_size].reshape(train_size, input_size) / 255.0

#Для того, чтобы было удобно сравнивать наши выходные значения со значениями картинок нам нужно их немного переработать: кодировать.
#Например, мы превратим число 5 в вектор: [0 0 0 0 0 1 0 0 0 0] Т.е. нужно создать матрицу, где мы преобразуем каждую цифру в вектор
#А выходные значения будем преобразовывать округлением: 0.01 = 0; 0.03 = 0; … 0.95 = 1. Получится, что мы будем работать с двумя одинаковыми векторами.
#Выполняем кодирование обозначений картинок из библиотеки mnist. Выбираем способ быстрого кодирования с помощью категориальной переменной.
"""Категориальная переменная - это переменная с ограниченным числом уникальных значений или категорий (например, пол или религия)."""
#Быстрое кодирование (One-Hot Encoding) – процесс, с помощью которого категориальные переменные преобразуются в подходящую ​​алгоритмам Машинного обучения (ML) форму.
#при создании любой Модели (Model), – это, как правило, предварительная подготовка данных (Data Preparation).
#Большая часть предварительной обработки – это кодирование в понятный компьютеру язык чисел. Отсюда и название 'encode', что буквально означает «преобразовать в [компьютерный] код». 
#Существует множество различных способов кодирования, таких как Ярлычное (Label Encoding) или Быстрое кодирование.

#Пример. Представьте, что у вас есть 3 категории продуктов: яблоки, курица и брокколи.
#Используя Ярлычное кодирование, вы должны присвоить каждому из них номер, чтобы разделить на категории: яблоки – 1, курица – 2 и брокколи – 3.
#Теперь, если Вашей модели предстоит рассчитать Среднее арифметическое (Mean), она по умолчанию сделает так: 1+ 3 = 4/2 = 2. 
#В соответствии с Вашей моделью, нечто среднее между яблоками и курицей – это брокколи. Обнаруженные корреляции будут совершенно неверны.
#Вместо того, чтобы обозначать вещи целыми положительными числами (1..3), мы воспользуемся бинарным стилем категоризации, 'One-Hot' – это как раз про это. 
#Визуализируем разницу между ярлыком и Быстрым кодированием. Акцентируйте внимание на разнице:
#Наши категории раньше были строками, теперь они столбцы. Однако наш числовой признак (Feature) – калории, остался прежним. 
#Ячейка A2 ('1') таблицы справа сообщит компьютеру категорию продукта правильным способом.
#Этот способ не всегда лучше Ярлычного кодирования, хотя бы потому, что является низкоэффективным способом хранения данных (число столбцов резко увеличивается).

#Итак кодируем наши обозначения
# One-Hot Encoding для меток
train_labels = np.zeros((train_size, output_size))#создаем нулевой массив, количество строк которого будет совпадать с количеством обозначений (10000), 
                                                  #а количество столбцов - с количеством цифер (10)
for i in range(train_size):
    train_labels[i, y_train[i]] = 1

# Подготовка тестовых данных
test_images = x_test[:test_size].reshape(test_size, input_size) / 255.0 #деление на 255 преобразует значения пикселей к диапазону от 0 до 1. 
#Так как x_train по сути содержит 60 000 изображений, а нам нужна только первая тысяча, то нам нужно изменить ее форму (размерность) с помощью функции reshape
test_labels = y_test[:test_size]
print(test_labels)

# Инициализация весов Генерируем матрицы весовых коэффициентов
#Для того, чтобы у нас все время были одни и теже коэффициенты зафиксируем диапазон рандомных значений.
np.random.seed(2)  # для воспроизводимости
weight_input_hidden = 0.2 * np.random.random((input_size, hidden_size)) - 0.1
weight_hidden_output = 0.2 * np.random.random((hidden_size, output_size)) - 0.1

# Зададим скорость обучения и количество эпох
learning_rate = 0.01
epochs = 100

# Обучение нейронной сети
for epoch in range(epochs):
    correct_predictions = 0
    
    for i in range(train_size):
        # Прямое распространение
        input_layer = train_images[i:i+1]
        hidden_layer = relu(np.dot(input_layer, weight_input_hidden))
        output_layer = np.dot(hidden_layer, weight_hidden_output)
      
        #np.argmax - находим индекс максимального элемента из массива полученных прогнозов
        # Подсчет правильных ответов
        predicted = np.argmax(output_layer)
        actual = np.argmax(train_labels[i:i+1])
        if predicted == actual:
            correct_predictions += 1
#Так мы просуммируем количество правильных ответов и потом поделим на количество изученных изображений всего и получим точность определения.
#Вычисляем значение дельты для корректировки марицы весов на скрытом и выходном слоях
        # Обратное распространение ошибки
        output_error = output_layer - train_labels[i:i+1]
        hidden_error = output_error.dot(weight_hidden_output.T) * relu_deriv(hidden_layer)
        
        # Обновление весов Определяем новые значения весов в матрицах весовых коэффициентов.
        weight_hidden_output -= learning_rate * hidden_layer.T.dot(output_error)
        weight_input_hidden -= learning_rate * input_layer.T.dot(hidden_error)
    
    # Вывод точности каждую эпоху
    accuracy = correct_predictions / train_size * 100
    print(f"Эпоха {epoch+1}/{epochs}, Точность: {accuracy:.2f}%")

# Тестирование на тестовых данных Проверяем точность работы НС на тестовом наборе данных.
correct_test = 0#будем подсчитывать количество правильных ответов
#начинаем стохастический градиентный спуск
for i in range(test_size):
    input_layer = test_images[i:i+1]
    hidden_layer = relu(np.dot(input_layer, weight_input_hidden))
    output_layer = np.dot(hidden_layer, weight_hidden_output)
    
    predicted = np.argmax(output_layer)
    if predicted == test_labels[i]:
        correct_test += 1

test_accuracy = correct_test / test_size * 100
print(f"\nТочность на тестовых данных: {test_accuracy:.2f}%")
#В случае, когда на тренировочном наборе данных точность вычисления НС доходит до 100%, то можно говорить о переобучении сети.
#Так обученная нейронная сеть на тестовых данных бычно дает прогноз с точностью около 60%. Так как тестовые значения отличаются от тренировочных. 
#Даже незначительные изменения приводят к низкому показателю точности определения.

# Пример предсказания для нескольких изображений
print("\nПримеры предсказаний:")
for i in range(5):
    input_layer = test_images[i:i+1]
    hidden_layer = relu(np.dot(input_layer, weight_input_hidden))
    output_layer = np.dot(hidden_layer, weight_hidden_output)
    
    predicted = np.argmax(output_layer)
    actual = test_labels[i]
    confidence = np.max(output_layer) * 100
    
    print(f"Изображение {i+1}: Предсказано {predicted}, Фактически {actual}, Уверенность: {confidence:.1f}%")


