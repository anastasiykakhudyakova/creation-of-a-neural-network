"""Функция активации softmax
— это обобщенная форма сигмоидальной функции для нескольких измерений. 
Это математическая функция, которая преобразует вектор чисел в вектор вероятностей.
Функция активации softmax обычно используется в качестве функции активации в случае задач многоклассовой классификации в машинном обучении. 
Выходные данные softmax интерпретируются как вероятность получения каждого класса.
Математическое выражение функции активации softmax имеет вид:
Т.е. это полезная функция активации выходного слоя нейронов в задачах где имеется несколько выходов, но только один из них является правильным.
Например, задачи классификации, в которых есть несколько вариантов классов и мы должны предсказать правильный.

Давайте изучим работу функции softmax на примере. Давайте рассмотрим нейронную сеть, которая классифицирует заданное изображение, будь то изображение кошки, собаки, тигра или ничего. 
Пусть X — вектор признаков т.е. X = [x1, x2, x3, x4] ).


Обычно мы используем функцию активации softmax в последнем слое нейронной сети,
— это весовая матрица. m = общее количество узлов в слое L-1 и n = количество узлов в выходном слое L. 
Вероятности принадлежности к индифферентным классам при заданных входных данных X рассчитываются следующим образом.

Преимущества использования функции активации softmax:
Может использоваться для многоклассовой классификации.
Он нормализует выходные данные для каждого класса от 0 до 1 и делит их на их сумму, давая вероятность того, что входное значение принадлежит определенному классу.
Для нейронных сетей, которым необходимо классифицировать входные данные по многочисленным категориям, Softmax часто используется исключительно для выходного слоя.


Софтмакс – функция, превращающая логиты (наборы чисел) в вероятности, причем сумма последних равна единице.
Функция выводит в качестве результата вектор, представляющий распределения вероятностей списка потенциальных результатов. 
Это также основной элемент, используемый в задачах Классификации (Classification) Глубокого обучения (Deep Learning).

В глубоком обучении термин "логитовый слой" обычно используется для последнего слоя Нейронной сети (Neural Network) задач классификации, 
которая преобразует необработанные значения прогноза в виде действительных чисел в диапазоне от [-∞, +∞]. Логиты – это необработанные результаты, 
полученные на последнем уровне нейронной сети до того, как произойдет активация.Softmax превращает логиты в вероятности, получая экспоненту e каждого значения,
а затем подвергая Нормализации (Normalization) каждое e, то есть разделяя на их сумму, чтобы сумма всех экспонент равнялась единице."""

#Так как наша задача суммировать строками, а не столбцами, то мы передаем 1 в axis и сохраняем размерность с помощью True как значение keepdims
#Рассмотрим функцию активации на примере преобразования числа из двоичной системы в десятичную

#задача функции активации softmax состоит в том, чтобы делать правильные ответы более ярко выраженными. 
#Она также как и функция sigmoid загоняет все ответы в диапазон от 0 до 1 для облегчения дальнейшей интерпретации результата.

"""Ключевые моменты реализации:
Softmax vs Sigmoid:
Sigmoid используется в скрытом слое для нелинейности
Softmax используется в выходном слое для получения вероятностей по классам
One-hot encoding:Каждый класс представляется вектором с одной единицей и остальными нулями
Инициализация весов:Используется масштабированная инициализация для лучшей сходимости
Функция потерь:Кросс-энтропия для многоклассовой классификации
Обучение:
Прямое распространение → вычисление ошибки → обратное распространение → обновление весов"""


 
"""Задания 
1.Напишите по памяти код "Функция активации softmax".
2.Добавьте ещё несколько входов для двоичного представления 10, 11, 12, 13, 14 и 15.
3.Измените соответствующие выходы, чтобы нейросеть могла выбирать число от 0 до 15, а не от 0 до 9
4/Убедитесь, что теперь нейросеть может корректно определять все числа от 0 до 15. 
Если это не так, то измените параметры нейросети: количество нейронов в скрытом слое, функцию активацию, количество эпох, скорость обучения."""
import numpy as np

# Определяем функцию активации сигмоида: она сжимает любое число в диапазон от 0 до 1
def sigmoid(x):
    # Ограничиваем значения x для предотвращения переполнения при вычислении экспоненты exp(-x)
    x = np.clip(x, -500, 500) #— это функция библиотеки NumPy, которая ограничивает значения массива в заданном диапазоне.
    # Возвращаем значение сигмоиды: 1 / (1 + e^(-x))
    return 1 / (1 + np.exp(-x))
# определяем производную сигмоиды, но принимаем уже готовое значение после сигмоиды (то есть s = sigmoid(z))
# производная сигмоиды: s * (1 - s), где s — выход функции sigmoid
# Определяем производную функции сигмоида (используется при обратном распространении ошибки)
def sigmoid_deriv(x):
    # x здесь — это уже результат работы sigmoid (s), производная вычисляется как s*(1-s)
    return x * (1 - x)#После реализации функции сигмоиды и расчета ее производной реализуем функцию softmax
# определяем функцию softmax: она превращает список чисел в вероятности (сумма = 1)
# Определяем функцию softmax для многоклассовой классификации
def softmax(x):#здесь x - это целый вектор
    # Ограничиваем значения для численной стабильности, чтобы не было ошибок при больших значениях
    x = np.clip(x, -500, 500) #np.clip() — это функция библиотеки NumPy, которая ограничивает значения массива в заданном диапазоне.
    # Вычитаем максимум из каждой строки для предотвращения переполнения экспоненты
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))#Рассчитываем экспоненту для входного вектора
#axis (ось) -Указывает, вдоль какой оси выполняется операция.
#keepdims (сохранять размерности) Булев параметр, который определяет, сохранять ли исходную размерность после операции.
    # Нормализуем: делим каждую экспоненту на сумму всех экспонент в строке
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)
  
# задаём входные данные: каждая строка — это двоичное представление числа от 0 до 15 (всего 16 чисел, по 4 бита) (двоичное представление чисел от 0 до 15 (4 бита на число))
x = np.array([
    [0, 0, 0, 0],  # 0 в двоичной системе
    [0, 0, 0, 1],  # 1
    [0, 0, 1, 0],  # 2
    [0, 0, 1, 1],  # 3
    [0, 1, 0, 0],  # 4
    [0, 1, 0, 1],  # 5
    [0, 1, 1, 0],  # 6
    [0, 1, 1, 1],  # 7
    [1, 0, 0, 0],  # 8
    [1, 0, 0, 1],  # 9
    [1, 0, 1, 0],  # 10
    [1, 0, 1, 1],  # 11
    [1, 1, 0, 0],  # 12
    [1, 1, 0, 1],  # 13
    [1, 1, 1, 0],  # 14
    [1, 1, 1, 1],  # 15
], dtype=float)  # Явно указываем тип данных как float для корректных вычислений

# Создаем целевые значения в формате one-hot encoding (16 классов)
y = np.eye(16)  # Создает единичную матрицу 16x16

# Определяем архитектуру нейросети
input_size = x.shape[1]   # Количество входных нейронов = 4 (4 бита)
hidden_size = 32          # Количество нейронов в скрытом слое (можно изменять)
output_size = y.shape[1]  # Количество выходных нейронов = 16 (классы 0-15)

# Фиксируем seed для воспроизводимости результатов
np.random.seed(42)

# Инициализируем веса между входным и скрытым слоем
# Используем инициализацию He с масштабированием sqrt(2/(fan_in + fan_out))
weight_hid = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / (input_size + hidden_size))

# Инициализируем веса между скрытым и выходным слоем
weight_out = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / (hidden_size + output_size))

# Задаем гиперпараметры обучения
learning_rate = 0.1  # Скорость обучения (шаг градиентного спуска)
epochs = 20000       # Количество эпох обучения (проходов по всем данным)

print("Обучение нейросети для распознавания чисел 0–15...")

# Начинаем процесс обучения
for epoch in range(epochs):
    # Прямое распространение (forward pass)
    z_hid = np.dot(x, weight_hid)        # Линейная комбинация на скрытом слое
    layer_hid = sigmoid(z_hid)           # Применяем функцию активации сигмоида
    
    z_out = np.dot(layer_hid, weight_out) # Линейная комбинация на выходном слое
    layer_out = softmax(z_out)            # Применяем softmax для получения вероятностей
    
    # Вычисление функции потерь (кросс-энтропия)
    eps = 1e-12  # Маленькое число для предотвращения log(0)
    # Средняя кросс-энтропия по всем примерам
    loss = -np.mean(np.sum(y * np.log(layer_out + eps), axis=1))
    #axis (ось) -Указывает, вдоль какой оси выполняется операция.
    # Обратное распространение ошибки (backward pass)
    # Градиент на выходном слое (упрощенная формула для softmax + cross-entropy)
    delta_out = layer_out - y
    
    # Градиент на скрытом слое (цепное правило)
    delta_hid = delta_out.dot(weight_out.T) * sigmoid_deriv(layer_hid)
    
    # Обновление весов методом градиентного спуска
    weight_out -= learning_rate * layer_hid.T.dot(delta_out)  # Веса выходного слоя
    weight_hid -= learning_rate * x.T.dot(delta_hid)          # Веса скрытого слоя
    
    # Вывод прогресса обучения каждые 2000 эпох
    if epoch % 2000 == 0:
        print(f"Эпоха: {epoch}, Потери: {loss:.6f}")

# Функция для предсказания на новых данных
def predict(inp):
    # Преобразуем вход в правильный формат
    inp = np.array(inp, dtype=float).reshape(1, -1)
    # Прямое распространение для одного примера
    layer_hid = sigmoid(np.dot(inp, weight_hid))
    layer_out = softmax(np.dot(layer_hid, weight_out))
    # Возвращаем класс с максимальной вероятностью и вектор вероятностей
    return np.argmax(layer_out), layer_out

print("\n=== Результаты предсказаний ===")

# Проверяем качество обучения на обучающей выборке
all_correct = True  # Флаг для отслеживания всех правильных предсказаний

# Проходим по всем примерам из обучающей выборки
for i, inp in enumerate(x):
    pred_class, probs = predict(inp)  # Получаем предсказание
    true_class = i                   # Истинный класс (индекс строки)
    correct = (pred_class == true_class)  # Проверяем правильность
    all_correct &= correct           # Обновляем общий флаг
    
    # Формируем строку статуса
    status = "ВЕРНО" if correct else "ОШИБКА"
    print(f"{status} Вход: {inp.astype(int)} → Истинный: {true_class}, "
          f"Предсказанный: {pred_class} (вероятность: {probs[0][pred_class]:.4f})")

# считаем итоговую точность: сколько примеров угадано верно из 16
accuracy = np.mean([predict(row)[0] == i for i, row in enumerate(x)])
# выводим точность в процентах
print(f"\nОбщая точность: {100 * accuracy:.1f}%")


# если все 16 примеров угаданы верно, выводим поздравление
if all_correct:
    print("Нейросеть идеально распознаёт все числа от 0 до 15!")
else:
    print("Есть ошибки. Попробуйте увеличить hidden_size или количество эпох.")
