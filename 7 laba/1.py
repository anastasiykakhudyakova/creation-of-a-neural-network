"""
Обучение со скрытыми слоями
Копируем предыдущую работу и на ее основе будем обучать нейронную сеть со скрытыми слоями.
"""

#задание 
"""
Напишите по памяти код из урока "Обучение со скрытыми слоями".
"""
# Импорт библиотеки numpy для работы с массивами и матрицами
import numpy as np
# Функция активации ReLU (Rectified Linear Unit)
def relu(x):
    # Альтернативная реализация с циклом 
    # for i in range(len(x)):
    #     if (x[i] < 0): x[i] = 0
    # return x 
    
    # Векторизованная реализация ReLU: возвращает x если x>0, иначе 0
    return (x > 0) * x

#Функция вычисления производной выглядит так:
# Производная функции ReLU для обратного распространения ошибки
def reluderiv(x):
    # Возвращает 1 для положительных значений, 0 для отрицательных
    return x > 0

# Входные данные: каждая строка - отдельный пример = Первый столбец - возраст, второй - какой-то другой признак
inp = np.array([[15, 10], [15, 18], [15, 20], [25, 10]])

# Целевые значения (правильные предсказания) для каждого примера
true_prediction = np.array([15, 18, 20, 25])

# Размер скрытого слоя (количество нейронов)
layer_hid_size = 3

# Размер входного слоя (количество признаков в каждом примере)
layer_in_size = len(inp[0])

# Размер выходного слоя (в нашем случае 1 нейрон - регрессия)
layer_out_size = 1

#Можно установить seed для randoma, чтобы получаемое значение блыо постоянным.# Инициализация весов случайными значениями
np.random.seed(100)  # Фиксируем seed для воспроизводимости результатов

# Веса между входным и скрытым слоем
# Инициализируем значения в диапазоне [-1, 1]
weight_hid = 2 * np.random.random((layer_in_size, layer_hid_size)) - 1

# Веса между скрытым и выходным слоем
weight_out = np.random.random((layer_hid_size, layer_out_size))

# Скорость обучения (шаг градиентного спуска)
learning_rate = 0.00001

# Количество эпох обучения (проходов по всем данным)
num_epoch = 100

# Основной цикл обучения
for i in range(num_epoch):
    layer_out_error = 0#задаем значение ошибки для вычисления дельты
    
    # Стохастический градиентный спуск: обрабатываем примеры по одному
    for i in range(len(inp)):
        # Входной слой: берем один пример (в виде матрицы 1x2)
        layer_in = inp[i:i+1] # получаем вот такое представление [[15 10]], т.е. не входной вектор значений, а входная матрица, состоящая из одной строки. Это необходимо для дальнейшего вычисления дельты матрицы весовых коэффициентов.
        # Прямое распространение 
        # Скрытый слой: линейная комбинация + функция активации ReLU
        layer_hid = relu(np.dot(layer_in, weight_hid))
        
        # Выходной слой: линейная комбинация (без функции активации для регрессии)
        layer_out = layer_hid.dot(weight_out)
        
        # Вычисление ошибки (сумма квадратов разностей)
        # Используем i:i+1 для сохранения размерности
        layer_out_error += np.sum((layer_out - true_prediction[i:i+1]) ** 2)#найдем сумму всех ошибок просуммировав значения ошибок всех выходных нейронов. В нашем случае он один, но np.sum делает функцию более универсальной на случай нескольких выходных нейронов

      #Корректируем матрицы весовых коэффициентов выходного слоя
        # Обратное распространение ошибки 
        # Дельты выходного слоя: разница между предсказанием и целевым значением
        layer_out_delta = true_prediction[i:i+1] - layer_out

#Теперьнужно понять как передать эту информацию об ошибке на скрытый слой. Т.е. нам
#нужно понять как именно каждый нейрон скрытого слоя повлиял на выходную ошибку.
#Для этого мы перемножаем дельту выходного слоя с транспонированной матрицей
#весовых коэффициентов на скрытом слое. Таким образом мы учтем размер веса определенногонейрона. 
  
#если значение весового коэффициента w1 было большим, а значит он внес больший вклад в получившуюся ошибку, то и изменять его надо сильно.
#А вот если значение весового коэффициента w2 незначительное и на ошибку почти не повлияло, то и изменять его нужно аккуратно.
#Но нельзя забывать, что при расчете прогноза скрытого слоя мы использовали функцию активации. И данную функцию тоже надо учитывать при расчете дельты.
#Например, если у нас нейрон скрытого слоя имеет нулевое значение, то какая разница на какой вес он умножается. 
#Поэтому нужно учитывать, что дельта должна быть нулевой для тех нейронов, которые не учавствовали в определении выходного прогноза.
#Т.е. по сути нам нужно помножить дельту на 0, если нейрон был нулевой, или на 1. В теории это есть производная функция ReLU.
      
        # Дельты скрытого слоя:
        # 1. Распространяем ошибку назад через веса (dot product с transposed weights_out)
        # 2. Умножаем на производную функции активации ReLU
        layer_hid_delta = layer_out_delta.dot(weight_out.T) * reluderiv(layer_hid)#layer_hid_delta = layer_out_delta.dot(weights_out.T) - вектор приращения для скрытого слоя
      
#Определяем приращения для весов
        # Обновление весов методом градиентного спуска
        weight_out += learning_rate * layer_hid.T.dot(layer_out_delta)
        weight_hid += learning_rate * layer_in.T.dot(layer_hid_delta)
        
        # Вывод текущего предсказания и целевого значения
        print("Predictions: %s, true_predictions: %s" % (layer_out, true_prediction[i:i+1]))
    
    # Вывод общей ошибки на текущей эпохе
    print("Errors: %.4f" % layer_out_error)
    print("----------------------")








"""задание1"""
# Импортируем библиотеку numpy для работы с массивами и математическими операциями
import numpy as np

# Определяем функцию активации ReLU: преобразует вход, оставляя положительные значения, остальные делает нулём
def relu(x):
    # Альтернативная реализация с циклом 
    # for i in range(len(x)):
    #     if (x[i] < 0): x[i] = 0
    # return x 
    # Векторизованная реализация ReLU: возвращает x если x>0, иначе 0
    return (x > 0) * x

# Определяем производную функции ReLU для использования в обратном распространении
def relu_derivative(x):
    # Возвращаем массив булевых значений: True (1) где x > 0, иначе False (0)
    return x > 0
# Входные данные: массив из 4 обучающих примеров, в каждом по 2 признака
inp = np.array([
    [15, 10],   # Пример 1
    [15, 18],   # Пример 2
    [15, 20],   # Пример 3
    [25, 10]    # Пример 4
])

# Правильные ответы (целевые значения) для каждого примера
true_prediction = np.array([15, 18, 20, 25])

# Определяем размер входного слоя: количество признаков в одном примере (2)
layer_in_size = len(inp[0])

# Устанавливаем количество нейронов в скрытом слое (3)
layer_hid_size = 3

# Устанавливаем количество выходных нейронов (1 для регрессии)
layer_out_size = 1

# Фиксируем seed генератора случайных чисел для воспроизводимости результатов
np.random.seed(100)

# Инициализируем веса между входным и скрытым слоями: матрица размером (2, 3)
# Значения генерируются в диапазоне [-1, 1]
weight_hid = 2 * np.random.random((layer_in_size, layer_hid_size)) - 1

# Инициализируем веса между скрытым и выходным слоями: матрица размером (3, 1)
# Значения генерируются в диапазоне [0, 1]
weight_out = np.random.random((layer_hid_size, layer_out_size))

# Устанавливаем скорость обучения (learning rate)
learning_rate = 0.0001

# Устанавливаем количество эпох (сколько раз пройдём по всем данным)
num_epochs = 100

# Основной цикл обучения: каждая эпоха — полный проход по всем обучающим примерам
for epoch in range(num_epochs):
    # Создаём накопитель суммарной ошибки за текущую эпоху
    total_error = 0
    
    # Цикл по каждому обучающему примеру (4 примера)
    for i in range(len(inp)):
        # Извлекаем i-й пример как одномерный массив в форме (1, 2) для матричных умножений
        layer_in = inp[i:i+1]
        
        # Вычисляем активации скрытого слоя: линейное преобразование + ReLU
        layer_hid = relu(layer_in.dot(weight_hid))
        
        # Вычисляем выход сети: линейное преобразование скрытого слоя (без активации на выходе для регрессии)
        layer_out = layer_hid.dot(weight_out)
        
        # Преобразуем истинное значение i-го примера в матрицу формы (1, 1) для согласования размеров
        true_val = true_prediction[i:i+1].reshape(1, 1)
        
        # Вычисляем квадрат ошибки для текущего примера (MSE, но без усреднения)
        error = np.sum((layer_out - true_val) ** 2)
        
        # Добавляем ошибку примера к общей ошибке эпохи
        total_error += error
        
        # Вычисляем градиент на выходном слое (разность между истиной и предсказанием)
        layer_out_delta = true_val - layer_out
        
        # Вычисляем градиент на скрытом слое:
        # 1. Переносим ошибку через веса выходного слоя (обратное умножение)
        # 2. Умножаем на производную ReLU (маску активных нейронов)
        layer_hid_delta = layer_out_delta.dot(weight_out.T) * relu_derivative(layer_hid)
        
        # Обновляем веса выходного слоя: прибавляем градиент, умноженный на скорость обучения
        weight_out += learning_rate * layer_hid.T.dot(layer_out_delta)
        
        # Обновляем веса скрытого слоя: прибавляем градиент, умноженный на скорость обучения
        weight_hid += learning_rate * layer_in.T.dot(layer_hid_delta)
# Вычислим финальную суммарную ошибку после обучения (повторяем прямой проход без обновления весов)
final_total_error = 0.0
for i in range(len(inp)):
    # Прямой проход для i-го примера
    layer_in = inp[i:i+1]
    layer_hid = relu(layer_in.dot(weight_hid))
    layer_out = layer_hid.dot(weight_out)
    
    # Получаем истинное значение
    true_val = true_prediction[i:i+1].reshape(1, 1)
    
    # Вычисляем ошибку
    error = np.sum((layer_out - true_val) ** 2)
    final_total_error += error
print(f"Финальная суммарная ошибка после обучения = {final_total_error:.8f}\n")

"""задание2"""
# Создаём универсальную функцию для обучения сети с заданным количеством нейронов в скрытом слое
def train_network(hidden_size, lr=0.0001, epochs=200, verbose=False):
    # Фиксируем случайное начальное состояние для воспроизводимости
    np.random.seed(100)
    
    # Инициализируем веса входного -> скрытого слоя размером (2, hidden_size)
    w_hid = 2 * np.random.random((layer_in_size, hidden_size)) - 1
    
    # Инициализируем веса скрытого -> выходного слоя размером (hidden_size, 1)
    w_out = np.random.random((hidden_size, layer_out_size))
    
    # Список для хранения истории ошибок по эпохам
    errors = []
    
    # Цикл по эпохам
    for epoch in range(epochs):
        # Сбрасываем накопленную ошибку в начале каждой эпохи
        total_error = 0.0
        
        # Цикл по обучающим примерам (стохастический градиентный спуск)
        for i in range(len(inp)):
            # Берём i-й входной пример
            x = inp[i:i+1]
            
            # Берём i-е истинное значение и приводим к форме (1,1)
            y_true = true_prediction[i:i+1].reshape(1, 1)
            
            # Прямой проход: скрытый слой с ReLU
            h = relu(x.dot(w_hid))
            
            # Прямой проход: выходной слой (линейная активация)
            y_pred = h.dot(w_out)
            
            # Вычисляем ошибку (MSE)
            error = np.sum((y_pred - y_true) ** 2)
            
            # Накапливаем ошибку
            total_error += error
            
            # Обратное распространение: градиент на выходе
            delta_out = y_true - y_pred
            
            # Обратное распространение: градиент на скрытом слое
            delta_hid = delta_out.dot(w_out.T) * relu_derivative(h)
            
            # Обновление весов выходного слоя по правилу градиентного спуска
            w_out += lr * h.T.dot(delta_out)
            
            # Обновление весов скрытого слоя по правилу градиентного спуска
            w_hid += lr * x.T.dot(delta_hid)
        
        # Сохраняем суммарную ошибку эпохи в историю
        errors.append(total_error)
    
    # Возвращаем итоговую ошибку и историю ошибок
    return total_error, errors

# Список размеров скрытого слоя для эксперимента
hidden_sizes = [2, 3, 5, 8, 12]

# Словарь для хранения результатов экспериментов
results = {}

# Цикл по разным размерам скрытого слоя
for size in hidden_sizes:
    # Обучаем сеть с текущим размером скрытого слоя
    final_err, _ = train_network(hidden_size=size, lr=0.0001, epochs=300)
    
    # Сохраняем результат в словарь
    results[size] = final_err
for size in hidden_sizes:
    print(f"Скрытый слой = {size:2d} → ошибка = {results[size]:.8f}")
print()


"""задание3"""
# Список скоростей обучения для тестирования
learning_rates = [0.00001, 0.0001, 0.001, 0.01]

# Фиксируем размер скрытого слоя для этого теста
hidden_size = 5

# Словарь для результатов задания 3
results_lr = {}

# Цикл по разным скоростям обучения
for lr in learning_rates:
    # Обучаем сеть с текущей скоростью обучения
    final_err, _ = train_network(hidden_size=hidden_size, lr=lr, epochs=200)
    results_lr[lr] = final_err
for lr in learning_rates:
    print(f"Скорость обучения = {lr:.5f} → ошибка = {results_lr[lr]:.8f}")
print()


"""задание4"""
# Список количества эпох для тестирования
epochs_list = [50, 100, 300, 1000]

# Фиксируем скорость обучения
lr = 0.0001

# Фиксируем размер скрытого слоя
hidden_size = 5

# Словарь для результатов задания 4
results_epochs = {}

# Цикл по различным количествам эпох
for epochs in epochs_list:
    # Обучаем сеть с заданным числом эпох
    final_err, _ = train_network(hidden_size=hidden_size, lr=lr, epochs=epochs)
    results_epochs[epochs] = final_err
for epochs in epochs_list:
    print(f"Эпох = {epochs:4d} → ошибка = {results_epochs[epochs]:.8f}")
