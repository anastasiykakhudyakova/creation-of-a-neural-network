"""
Обучение со скрытыми слоями
Копируем предыдущую работу и на ее основе будем обучать нейронную сеть со скрытыми слоями.
"""

#задание 
"""
Напишите по памяти код из урока "Обучение со скрытыми слоями".
"""
# Импорт библиотеки numpy для работы с массивами и матрицами
import numpy as np
# Функция активации ReLU (Rectified Linear Unit)
def relu(x):
    # Альтернативная реализация с циклом 
    # for i in range(len(x)):
    #     if (x[i] < 0): x[i] = 0
    # return x 
    
    # Векторизованная реализация ReLU: возвращает x если x>0, иначе 0
    return (x > 0) * x

#Функция вычисления производной выглядит так:
# Производная функции ReLU для обратного распространения ошибки
def reluderiv(x):
    # Возвращает 1 для положительных значений, 0 для отрицательных
    return x > 0

# Входные данные: каждая строка - отдельный пример = Первый столбец - возраст, второй - какой-то другой признак
inp = np.array([[15, 10], [15, 18], [15, 20], [25, 10]])

# Целевые значения (правильные предсказания) для каждого примера
true_prediction = np.array([15, 18, 20, 25])

# Размер скрытого слоя (количество нейронов)
layer_hid_size = 3

# Размер входного слоя (количество признаков в каждом примере)
layer_in_size = len(inp[0])

# Размер выходного слоя (в нашем случае 1 нейрон - регрессия)
layer_out_size = 1

#Можно установить seed для randoma, чтобы получаемое значение блыо постоянным.# Инициализация весов случайными значениями
np.random.seed(100)  # Фиксируем seed для воспроизводимости результатов

# Веса между входным и скрытым слоем
# Инициализируем значения в диапазоне [-1, 1]
weight_hid = 2 * np.random.random((layer_in_size, layer_hid_size)) - 1

# Веса между скрытым и выходным слоем
weight_out = np.random.random((layer_hid_size, layer_out_size))

# Скорость обучения (шаг градиентного спуска)
learning_rate = 0.00001

# Количество эпох обучения (проходов по всем данным)
num_epoch = 100

# Основной цикл обучения
for i in range(num_epoch):
    layer_out_error = 0#задаем значение ошибки для вычисления дельты
    
    # Стохастический градиентный спуск: обрабатываем примеры по одному
    for i in range(len(inp)):
        # Входной слой: берем один пример (в виде матрицы 1x2)
        layer_in = inp[i:i+1] # получаем вот такое представление [[15 10]], т.е. не входной вектор значений, а входная матрица, состоящая из одной строки. Это необходимо для дальнейшего вычисления дельты матрицы весовых коэффициентов.
        # Прямое распространение 
        # Скрытый слой: линейная комбинация + функция активации ReLU
        layer_hid = relu(np.dot(layer_in, weight_hid))
        
        # Выходной слой: линейная комбинация (без функции активации для регрессии)
        layer_out = layer_hid.dot(weight_out)
        
        # Вычисление ошибки (сумма квадратов разностей)
        # Используем i:i+1 для сохранения размерности
        layer_out_error += np.sum((layer_out - true_prediction[i:i+1]) ** 2)#найдем сумму всех ошибок просуммировав значения ошибок всех выходных нейронов. В нашем случае он один, но np.sum делает функцию более универсальной на случай нескольких выходных нейронов

      #Корректируем матрицы весовых коэффициентов выходного слоя
        # Обратное распространение ошибки 
        # Дельты выходного слоя: разница между предсказанием и целевым значением
        layer_out_delta = true_prediction[i:i+1] - layer_out

#Теперьнужно понять как передать эту информацию об ошибке на скрытый слой. Т.е. нам
#нужно понять как именно каждый нейрон скрытого слоя повлиял на выходную ошибку.
#Для этого мы перемножаем дельту выходного слоя с транспонированной матрицей
#весовых коэффициентов на скрытом слое. Таким образом мы учтем размер веса определенногонейрона. 
  
#если значение весового коэффициента w1 было большим, а значит он внес больший вклад в получившуюся ошибку, то и изменять его надо сильно.
#А вот если значение весового коэффициента w2 незначительное и на ошибку почти не повлияло, то и изменять его нужно аккуратно.
#Но нельзя забывать, что при расчете прогноза скрытого слоя мы использовали функцию активации. И данную функцию тоже надо учитывать при расчете дельты.
#Например, если у нас нейрон скрытого слоя имеет нулевое значение, то какая разница на какой вес он умножается. 
#Поэтому нужно учитывать, что дельта должна быть нулевой для тех нейронов, которые не учавствовали в определении выходного прогноза.
#Т.е. по сути нам нужно помножить дельту на 0, если нейрон был нулевой, или на 1. В теории это есть производная функция ReLU.
      
        # Дельты скрытого слоя:
        # 1. Распространяем ошибку назад через веса (dot product с transposed weights_out)
        # 2. Умножаем на производную функции активации ReLU
        layer_hid_delta = layer_out_delta.dot(weight_out.T) * reluderiv(layer_hid)#layer_hid_delta = layer_out_delta.dot(weights_out.T) - вектор приращения для скрытого слоя
      
#Определяем приращения для весов
        # Обновление весов методом градиентного спуска
        weight_out += learning_rate * layer_hid.T.dot(layer_out_delta)
        weight_hid += learning_rate * layer_in.T.dot(layer_hid_delta)
        
        # Вывод текущего предсказания и целевого значения
        print("Predictions: %s, true_predictions: %s" % (layer_out, true_prediction[i:i+1]))
    
    # Вывод общей ошибки на текущей эпохе
    print("Errors: %.4f" % layer_out_error)
    print("----------------------")
