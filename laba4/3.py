#Градиентный спуск с несколькими выходами
#теперь у нас есть вектор ошибок. и на каждой итерации весовые коэффиенты рассчитываются независимо друг от друга.

"""задание"""
#Напишите по памяти код из урока "Градиентный спуск с несколькими выходами".
# Импорт библиотеки NumPy для работы с массивами и математическими операциями
import numpy as np

# Функция нейронной сети с несколькими выходами
# Принимает скалярный вход и вектор весов, возвращает вектор выходов
def neural_networks(inp, weights):
    return inp * weights  # Умножение входа на каждый вес (broadcasting в NumPy)


# Функция для вычисления ошибки
# Работает с векторами: вычисляет сумму квадратов разностей
def get_error(true_prediction, prediction):
    return (true_prediction - prediction) ** 2  # Поэлементное возведение в квадрат


# Функция градиентного спуска для нескольких выходов
def gradient(inp, weights, true_predictions, count_iters, learning_rate):
    # Цикл обучения на заданное количество итераций
    for i in range(count_iters):
        # Шаг 1: Прямое распространение (forward pass)
        prediction = neural_networks(inp, weights)  # Получаем предсказания для всех выходов
        
        # Шаг 2: Вычисление ошибки для каждого выхода
        error = get_error(true_predictions, prediction)  # Вектор ошибок для каждого выхода
        
        # Шаг 3: Вывод отладочной информации
        print("Prediction: %s, Weights: %s, Error: %s" % (prediction, weights, error))
        
        # Шаг 4: Вычисление градиента для каждого веса
        # Формула: производная от MSE = 2*(prediction - true_prediction)*inp
        # (множитель 2 опущен, включен в learning_rate)
        delta = (prediction - true_predictions) * inp * learning_rate
        
        # Шаг 5: Обновление всех весов одновременно
        weights = weights - delta  # Градиентный спуск для каждого веса


# Определение входного значения (скаляр)
inp = 150  # Один входной признак (например, рост человека)

# Определение весов для двух выходов (вектор)
weights = np.array([0.2, 0.3])  # Два веса: первый для первого выхода, второй для второго

# Определение целевых значений для двух выходов (вектор)
true_predictions = np.array([50, 120])  # Два целевых значения:
                                        # - первый выход должен предсказать 50
                                        # - второй выход должен предсказать 120

# Скорость обучения (learning rate)
learning_rate = 0.00001  # Маленький шаг для стабильного обучения

# Количество итераций обучения
count_iters = 30  # 30 шагов градиентного спуска

# Запуск процесса обучения
gradient(inp, weights, true_predictions, count_iters, learning_rate)





"""задание"""
#Измените inp на другие значения (например, inp = 200). 
#Как изменение входных данных влияет на процесс обучения? Какие значения inp делают обучение более сложным, а какие более простым?
"""
Значения inp, которые слишком велики, могут сделать обучение более сложным, 
так как большие входные значения увеличивают величину шага обновления весов, 
что может привести к скачкам весов и затруднению сходимости. 
Слишком маленькие значения inp могут замедлить обучение, 
потому что величина обновления весов становится очень малой.

Большие значения inp - потенциальная нестабильность.
Нужно лучше адаптировать шаг обучения.
"""
import numpy as np
# Функция нейронной сети с несколькими выходами
# Умножает входное значение на каждый вес для получения предсказаний
def neural_networks(inp, weights):
    return inp * weights  # Broadcasting: inp (скаляр) умножается на каждый элемент weights


# Функция для вычисления ошибки (поэлементно для каждого выхода)
def get_error(true_prediction, prediction):
    return (true_prediction - prediction) ** 2  # Квадрат разности между целью и предсказанием


# Функция градиентного спуска для нескольких выходов
def gradient(inp, weights, true_predictions, count_iters, learning_rate):
    # Цикл обучения на заданное количество итераций
    for i in range(count_iters):
        # Шаг 1: Получение предсказаний
        prediction = neural_networks(inp, weights)
        
        # Шаг 2: Вычисление ошибок для каждого выхода
        error = get_error(true_predictions, prediction)
        
        # Шаг 3: Вывод информации о текущем состоянии
        print("Prediction: %s, Weights: %s, Error: %s" % (prediction, weights, error))
        
        # Шаг 4: Вычисление градиента
        # Формула: delta = (предсказание - цель) * вход * скорость_обучения
        delta = (prediction - true_predictions) * inp * learning_rate
        
        # Шаг 5: Обновление весов (градиентный спуск)
        weights = weights - delta


# Измененное входное значение (вместо 150 теперь 200)
inp = 200  # Увеличенное значение входного признака

# Веса для двух выходов (остаются прежними)
weights = np.array([0.2, 0.3])  # Начальные значения весов

# Целевые значения для двух выходов
true_predictions = np.array([50, 120])  # Неизменные целевые значения

# Скорость обучения (остается прежней)
learning_rate = 0.00001  # Маленькое значение для устойчивого обучения

# Количество итераций обучения
count_iters = 30  # 30 шагов градиентного спуска

# Запуск процесса обучения с новым входным значением
gradient(inp, weights, true_predictions, count_iters, learning_rate)

"""
ИСХОДНЫЙ СЛУЧАЙ (inp = 150):
- Начальные предсказания: 150*0.2=30, 150*0.3=45
- Разность с целями: 50-30=20, 120-45=75
- Градиент для weight1: (30-50)*150 = -20*150 = -3000
- Градиент для weight2: (45-120)*150 = -75*150 = -11250

НОВЫЙ СЛУЧАЙ (inp = 200):
- Начальные предсказания: 200*0.2=40, 200*0.3=60
- Разность с целями: 50-40=10, 120-60=60
- Градиент для weight1: (40-50)*200 = -10*200 = -2000
- Градиент для weight2: (60-120)*200 = -60*200 = -12000

ВЛИЯНИЕ НА ПРОЦЕСС ОБУЧЕНИЯ:

1. НАЧАЛЬНЫЕ ПРЕДСКАЗАНИЯ:
   - При inp=200: [40, 60] vs цели [50, 120]
   - При inp=150: [30, 45] vs цели [50, 120]
   - Увеличение inp делает начальные предсказания ближе к целям для первого выхода

2. ВЕЛИЧИНА ГРАДИЕНТА:
   - Градиент пропорционален inp: delta ∝ inp
   - При inp=200: шаг обновления весов в 200/150 ≈ 1.33 раза больше
   - Это может ускорить обучение, но требует осторожности

3. КАКИЕ ЗНАЧЕНИЯ inp ДЕЛАЮТ ОБУЧЕНИЕ СЛОЖНЕЕ:

   СЛИШКОМ БОЛЬШИЕ inp (например, 1000+):
   - Градиент становится очень большим: delta ∝ inp
   - Риск расходимости и "перескока" через оптимальные значения
   - Пример: если inp=1000, шаг будет в 5 раз больше, чем при inp=200
   - Может потребоваться уменьшение learning_rate

   СЛИШКОМ МАЛЕНЬКИЕ inp (например, 10):
   - Градиент становится очень маленьким: delta ∝ inp
   - Обучение замедляется, требуется больше итераций
   - Пример: если inp=10, шаг будет в 20 раз меньше, чем при inp=200
