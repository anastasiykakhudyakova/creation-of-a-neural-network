"""Обучение на нескольких наборах данных"""
#весовые коэффициенты меняются на каждом наборе данных.
"""стохастический градиентный спуск."""
#Такой подход, при котором мы меняем значения весовых коэффициентов на каждом наборе входных данных называется 
#Есть еще один вариант градиентного спуска – это полный градиентный спуск. Здесь мы находим среднее значение дельты и только потом меняем весовые коэффициенты.
#Фактически весовая матрица изменится только после того как мы все данные проверили, нашли все три дельты, просуммировали их поделили на три.
#Есть третий вариант градиентного спуска – это пакетный градиентный спуск. К примеру, имеем не три набора данных, а 500.
#Здесь мы можем для каждых ста наборов данных найти средне арифметическое значение дельты и потом отрегулировать весовый коэффициенты. Далее берем следующую сотню и так далее.
#Стохастический градиентный спуск используется очень часто и наиболее популярен, но и пакетный испоьзуется достаточно часто.
#Осталось определить ошибку после обучения на всех наборах данных.при большой ошибке можно увеличить количество эпох или увелить скорость обучения.
#Чем больше наборов данных тем лучше результат, но можно и переобучить нейронную сеть.
#переобученная нейросеть при малейшем несоответствии шаблону отказывается работать и говорит, что все входные данные не подходят. 
#Т.е. на шаблонных данных она будет показывать отличные результаты с минимальной ошибкой, но чуть отличающиеся приведут к ошибке системы. Об переобучении будем говорить далее.

"""задание"""
#Напишите по памяти код из урока "Обучение на нескольких наборах данных".
import numpy as np

# Функция нейронной сети (простая линейная модель)
def neural_networks(inp, weights):
    # Вычисляем скалярное произведение входного вектора на веса
    # Это прямое распространение сигнала через нейрон
    return inp.dot(weights)

# Функция вычисления ошибки
def get_error(true_prediction, prediction):
    # Вычисляем квадрат разности между реальным и предсказанным значением
    # Это среднеквадратичная ошибка (MSE) для одного примера
    return (true_prediction - prediction) ** 2

# Функция градиентного спуска для обучения на нескольких наборах данных
def gradient(inp, weights, true_predictions, count_iters, learning_rate):
    # Внешний цикл: количество эпох (полных проходов по всем данным)
    for i in range(count_iters):
        # Внутренний цикл: итерация по всем примерам в наборе данных
        for j in range(len(inp)):
            # Выбираем текущий входной вектор из набора данных
            cur_inp = inp[j]
            
            # Выбираем соответствующее целевое значение для текущего примера
            cur_predict = true_predictions[j]
            
            # Прямой проход: получаем предсказание для текущего примера
            prediction = neural_networks(cur_inp, weights)
            
            # Вычисляем ошибку для текущего примера
            error = get_error(cur_predict, prediction)
            
            # Выводим отладочную информацию
            print("Prediction: ", prediction)  # Предсказание модели
            print("Weights: ", weights)        # Текущие веса модели
            print("Error: ", error)            # Ошибка на текущем примере
            print("-------------------")       # Разделитель для читаемости
            
            # Вычисляем градиент (поправку к весам) для текущего примера
            # Формула: (предсказание - цель) * вход * скорость_обучения
            # Это производная функции ошибки по весам
            delta = (prediction - cur_predict) * cur_inp * learning_rate
            
            # Обновляем веса методом градиентного спуска
            # Вычитаем градиент для минимизации ошибки
            weights = weights - delta

# Определение входных данных 
inp = np.array([[150, 40], [170, 80], [160, 90]])

# Определение целевых значений (ожидаемых выходов) для каждого примера
true_predictions = np.array([50, 120, 140])

# Инициализация весов модели (два веса для двух входных признаков)
weights = np.array([0.2, 0.3])

# Гиперпараметры обучения
count_iters = 50         # Количество эпох обучения
learning_rate = 0.00001  # Скорость обучения (размер шага при обновлении весов)

# Запуск процесса обучения
# Передаем все данные: входы, начальные веса, цели, количество эпох и скорость обучения
gradient(inp, weights, true_predictions, count_iters, learning_rate)



"""задание"""
#Измените количество эпох с 500 на 100. Проанализируйте результат.
import numpy as np
# Функция нейронной сети (простая линейная модель - один нейрон)
def neural_networks(inp, weights):
    # Вычисляем скалярное произведение входного вектора на веса
    return inp.dot(weights)
  
# Функция для вычисления ошибки предсказания
def get_error(true_prediction, prediction):
    # Вычисляем квадратичную ошибку (MSE для одного примера)
    # Квадрат разности между фактическим и предсказанным значением
    return (true_prediction - prediction) ** 2


# Функция градиентного спуска для обучения на нескольких примерах
def gradient(inp, weights, true_predictions, count_iters, learning_rate):
    # Внешний цикл: количество эпох (полных проходов по всему набору данных)
    for i in range(count_iters):
        # Внутренний цикл: проход по каждому примеру в наборе данных
        for j in range(len(inp)):
            # Извлекаем j-й входной вектор (признаки текущего примера)
            cur_inp = inp[j]
            
            # Извлекаем соответствующее целевое значение для j-го примера
            cur_predict = true_predictions[j]
            
            # Прямой проход (forward propagation): получаем предсказание
            prediction = neural_networks(cur_inp, weights)
            
            # Вычисляем ошибку для текущего предсказания
            error = get_error(cur_predict, prediction)
            
            # Выводим диагностическую информацию для отслеживания обучения
            print("Prediction: ", prediction)  # Текущее предсказание модели
            print("Weights: ", weights)        # Текущие значения весов
            print("Error: ", error)            # Текущая ошибка
            print("-------------------")       # Разделитель для удобства чтения
            
            # Вычисляем градиент (вектор поправок к весам)
            # Формула: delta = (prediction - target) * input * learning_rate
            # Это производная функции ошибки по весам, умноженная на скорость обучения
            delta = (prediction - cur_predict) * cur_inp * learning_rate
            
            # Обновляем веса методом градиентного спуска
            # weights = weights - delta  (сокращенная запись)
            weights -= delta


# Создаем входные данные (матрица признаков)
# Три примера, каждый с двумя признаками (например: [рост, возраст]
inp = np.array([[150, 40], [170, 80], [160, 90]])

# Целевые значения для каждого примера 
true_predictions = np.array([50, 120, 140])

# Инициализация весов модели (два веса для двух входных признаков)
weights = np.array([0.2, 0.3])

# Установка гиперпараметров обучения:
count_iters = 100        # ИЗМЕНЕНО: количество эпох уменьшено с 500 до 100
learning_rate = 0.0001   # Скорость обучения (размер шага при обновлении весов)
gradient(inp, weights, true_predictions, count_iters, learning_rate)




"""задание"""
#Измените функцию get_error так, чтобы она возвращала не сумму квадратов ошибок, а среднеквадратичное отклонение (RMSE): np.sqrt(np.mean((true_value - prediction_value) ** 2)).
#Как это изменение влияет на обучение нейросети?
"""Удобнее контроллировать процесс обучения модели."""
import numpy as np

# Функция нейронной сети (простая линейная модель)
def neural_networks(inp, weights):
    # Вычисляем скалярное произведение входного вектора и весов
    # Это линейная комбинация входных признаков
    return inp.dot(weights)
  
def get_error(true_prediction, prediction):
    # Формула RMSE: sqrt(mean((true - pred)^2))
    # 1. (true_prediction - prediction) ** 2 - квадрат разности
    # 2. np.mean() - вычисление среднего значения (для одного числа вернет само число)
    # 3. np.sqrt() - извлечение квадратного корня
    return np.sqrt(np.mean((true_prediction - prediction) ** 2))#  - среднеквадратичное отклонение


# Функция градиентного спуска для обучения модели
def gradient(inp, weights, true_predictions, count_iters, learning_rate):
    # Внешний цикл по количеству эпох (полных проходов по данным)
    for i in range(count_iters):
        # Внутренний цикл: проход по каждому примеру в наборе данных
        for j in range(len(inp)):
            # Получаем j-й входной вектор из матрицы входных данных
            cur_inp = inp[j]
            
            # Получаем соответствующее целевое значение для j-го примера
            cur_predict = true_predictions[j]
            
            # Прямой проход - получаем предсказание модели
            prediction = neural_networks(cur_inp, weights)
            
            # Вычисляем ошибку между предсказанием и целевым значением
            error = get_error(cur_predict, prediction)
            
            # Выводим диагностическую информацию на каждой итерации
            print("Prediction: ", prediction)  # Текущее предсказание
            print("Weights: ", weights)        # Текущие значения весов
            print("Error: ", error)            # Ошибка (в шкале RMSE)
            print("-------------------")       # Разделитель для удобства чтения
            
            # Вычисляем градиент (вектор поправок к весам)
            # Формула: (предсказание - целевое_значение) * входной_вектор * скорость_обучения
            delta = (prediction - cur_predict) * cur_inp * learning_rate
            
            # Обновляем веса методом градиентного спуска
            # Вычитаем градиент для минимизации ошибки
            weights -= delta

# Создаем матрицу входных данных (3 примера, 2 признака каждый)
inp = np.array([[150, 40], [170, 80], [160, 90]])

# Создаем вектор целевых значений (по одному на каждый пример)
true_predictions = np.array([50, 120, 140])

# Инициализируем вектор весов (начальные значения)
# Два веса для двух входных признаков
weights = np.array([0.2, 0.3])

# Устанавливаем гиперпараметры обучения
count_iters = 100        # Количество эпох (полных проходов по всем данным)
learning_rate = 0.0001   # Скорость обучения (размер шага при обновлении весов)

# Запускаем процесс обучения
gradient(inp, weights, true_predictions, count_iters, learning_rate)
