# Обучение на нескольких наборах данных
'''Решение проблемы с расхождением'''
# Одна из проблем градиентного спуска - решение проблемы с расхождением
# Расхождение случается когда происходит резкое изменение веса. Вес в свою очередь сильно меняется, когда идет огромная ошибка и как следствие мы получаем огромную производную.
# В результате мы получаем огромную производную, а значит необходимо резко изменить вес.
# Меняя вес мы проскакиваем искомое значение веса и устремляемся вверх параболы на другой ее стороне. Снова получаем огромную ошибку и так уходим в бесконечный цикл подбора веса.
# Для решения проблемы расхождения вносят так называемый α-коэффициент или learning rate (скорость обучения).
# Данный коэффициент подбирается в зависимости от текущего результата обучения.
# Если сделать скорость обучения еще меньше, то за 10 эпох сеть так ничему и не научится. Опять же если сделать слишком большой, то система снова сломается. Нужен баланс

'''Задание'''
# Напишите по памяти код из урока "Решение проблемы с расхождением".
import numpy as np #Импорт библиотеки numpy для математических операций

# Функция нейронной сети - простое линейное преобразование
def neural_networks(inp, weight):
    return inp * weight  # Умножаем вход на вес для получения предсказания

# Функция вычисления ошибки - квадрат разности между реальным значением и предсказанием
def get_error(true_prediction, prediction):
    return (true_prediction - prediction) ** 2  # Квадратичная функция ошибки

# Функция градиентного спуска
def gradient(inp, weight, true_prediction, n):
    learning_rate = 0.001  # Коэффициент скорости обучения (малое значение для стабильности)
    for i in range(n):  # Цикл по количеству итераций обучения
        prediction = neural_networks(inp, weight)  # Получаем предсказание от нейронной сети
        error = get_error(true_prediction, prediction)  # Вычисляем ошибку предсказания
        # Вывод текущих значений предсказания, веса и ошибки с заданной точностью
        print("Prediction: %.10f, Weight: %.5f, Error: %.20f" %(prediction, weight, error))
        # Вычисление изменения веса: производная ошибки по весу, умноженная на learning_rate
        delta = (prediction - true_prediction) * inp * learning_rate  # Умножаем нашу производную на скорость обучения
        weight = weight - delta  # Обновляем вес с учетом вычисленного изменения
gradient(30, 0.2, 70, 10) #Вызов функции градиентного спуска с параметрами: вход=30, начальный вес=0.2, целевое значение=70, итераций=10

'''Задание'''
# Измените значение learning_rate на 0.01, 0.1 и 0.0001. 
# Как это влияет на скорость обучения?
# Попробуйте найти оптимальное значение learning_rate, которое обеспечивает быстрое и точное обучение.
import numpy as np
# Функция нейронной сети - линейная модель
def neural_networks(inp, weight):
    return inp * weight  # Простое умножение входа на вес

# Функция вычисления квадратичной ошибки
def get_error(true_prediction, prediction):
    return (true_prediction - prediction) ** 2  # Квадрат разности между целевым и предсказанным значениями

# Функция градиентного спуска с параметром learning_rate
def gradient(inp, weight, true_prediction, count_iters, learning_rate):
    for i in range(count_iters):  # Цикл по заданному количеству итераций
        prediction = neural_networks(inp, weight)  # Получаем текущее предсказание
        error = get_error(true_prediction, prediction)  # Вычисляем ошибку
        # Вывод информации о текущем состоянии обучения
        print("Prediction: %.10f, Weight: %.5f, Error: %.20f" %(prediction, weight, error))
        # Вычисление градиента и изменение веса с учетом learning_rate
        delta = (prediction - true_prediction) * inp * learning_rate  # Изменение веса
        weight = weight - delta  # Обновление веса

print("1")  # Метка для вывода первого эксперимента
gradient(30, 0.2, 70, 20, 0.01)  # learning_rate=0.01: обучение ломается (слишком большой шаг)
print("2")  # Метка для вывода второго эксперимента
gradient(30, 0.2, 70, 20, 0.1)  # learning_rate=0.1: обучение ломается (очень большой шаг)
print("3")  # Метка для вывода третьего эксперимента
gradient(30, 0.2, 70, 20, 0.0001)  # learning_rate=0.0001: слишком медленное обучение
print("4")  # Метка для вывода четвертого эксперимента
gradient(30, 0.2, 70, 20, 0.001)  # learning_rate=0.001: подходящий коэффициент

'''Задание'''
# Увеличьте количество итераций цикла (например, до 100).
# Как это влияет на точность предсказаний? 
# Сколько итераций необходимо для достижения близкой к идеальной точности предсказаний?
import numpy as np
# Функция нейронной сети
def neural_networks(inp, weight):
    return inp * weight  # Линейная модель

# Функция вычисления ошибки
def get_error(true_prediction, prediction):
    return (true_prediction - prediction) ** 2  # Квадратичная функция потерь

# Функция градиентного спуска
def gradient(inp, weight, true_prediction, count_iters, learning_rate):
    for i in range(count_iters):  # Цикл обучения на заданное количество итераций
        prediction = neural_networks(inp, weight)  # Текущее предсказание модели
        error = get_error(true_prediction, prediction)  # Ошибка предсказания
        # Вывод статистики обучения
        print("Prediction: %.10f, Weight: %.5f, Error: %.20f" %(prediction, weight, error))
        # Вычисление изменения веса
        delta = (prediction - true_prediction) * inp * learning_rate  # Градиент, умноженный на скорость обучения
        weight = weight - delta  # Корректировка веса

# Слишком медленное обучение из-за очень маленького learning_rate
# Поэтому нужно больше итераций для достижения хорошего результата
gradient(30, 0.2, 70, 100, 0.0001)  # 100 итераций с learning_rate=0.0001
print()  # Пустая строка для разделения выводов

# Подходящий коэффициент скорости обучения
# Можно использовать меньше итераций для достижения того же результата
gradient(30, 0.2, 70, 20, 0.001)  # 20 итераций с learning_rate=0.001
# Чем меньше learning_rate, тем больше нужно итераций для обучения
