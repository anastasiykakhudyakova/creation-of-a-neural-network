"""Градиентный спуск с несколькими входами и выходами"""
#обратное распространение, т.е. мы берем выход и начинаем как бы идти назад настраиваевая весовую матрицу, чтобы обучить нейросеть
#Обратное распространение – регулирование матрицы весовых коэффициентов на основании ее входов.

"""задание"""
#Напишите по памяти код из урока "Градиентный спуск с несколькими входами и несколькими выходами".
import numpy as np

# Функция нейронной сети (фактически - линейная модель)
def neural_network(input, weights):
    # Возвращаем скалярное произведение входного вектора на весовую матрицу
    # Это прямое распространение сигнала через нейронную сеть
    return input.dot(weights)

# Реализуем функцию, возвращающую ошибку прогнозирования
def get_error(true_prediction, prediction):
    # Вычисляем среднеквадратичную ошибку (MSE) для одного примера
    # (разность между реальным и предсказанным значением в квадрате)
    return (true_prediction - prediction) ** 2

# Функция градиентного спуска для обучения нейронной сети
def gradient(inp, weights, true_predictions, count_iters, learning_rate):
    # Основной цикл обучения: повторяем count_iters раз
    for i in range(count_iters):
        # Прямое распространение: получаем предсказание сети
        prediction = neural_network(inp, weights)  # ПРЯМОЕ РАСПРОСТРАНЕНИЕ
        
        # Вычисляем ошибку между предсказанием и реальными значениями
        error = get_error(true_predictions, prediction)
        
        # Выводим информацию о текущей итерации
        print("Prediction: %s, Weights: %s, Error: %s" % (prediction, weights, error))
        
        # Вычисляем градиент (вектор поправок к весам)
        # Формула: delta = (prediction - true) * input * learning_rate
        # Это производная функции ошибки по весам (с учетом learning_rate)
        delta = (prediction - true_predictions) * inp * learning_rate
        
        # Обновляем веса: вычитаем градиент (градиентный спуск)
        weights = weights - delta  # ОБРАТНОЕ РАСПРОСТРАНЕНИЕ, т.е. мы берем выход и начинаем как бы идти назад настраиваевая весовую матрицу, чтобы обучить нейросеть

# ОБЪЯВЛЯЕМ ПАРАМЕТРЫ ДЛЯ ОБУЧЕНИЯ
# Входной вектор: [рост в см, возраст в годах]
inp = np.array([150, 40])

# Весовая матрица: преобразует 2 входа в 2 выхода
# Изначально случайные веса, затем транспонируем для правильной размерности
weights = np.array([[0.2, 0.3],[0.5, 0.7]]).T  # Транспонируем матрицу весов

# Ожидаемые (целевые) значения выхода
true_predictions = np.array([50, 120])

# Скорость обучения: определяет размер шага при обновлении весов
# Слишком большое значение может привести к расходимости,
# слишком маленькое - к медленному обучению
learning_rate = 0.00001

# Количество итераций обучения (эпох)
count_iters = 50

# Вызываем функцию градиентного спуска со всеми параметрами
gradient(inp, weights, true_predictions, count_iters, learning_rate)



"""задание"""
#Измените true_predictions на другие значения (например, [[70, 90]] или [[30, 110]]).
#Запустите код и определите, как это влияет на обучение нейросети.
#Какие значения true_predictions делают обучение более сложным, а какие более простым?
"""
1) Большое расхождение с предсказаниями
    Большие отличия от входных значений приводят к нестабильности.
    Модели трудно достичь больших значений при небольших входах.
    
2) Малые значения увеличивают величину ошибки. Это требует уменьшение скорости обучения
    Если целевые значения меньше предсказаний, ошибка всё равно большая,
    но может потребоваться меньшая скорость обучения для стабильности.
    
3) Разброс значений приводит к более сложному обучению.
    Когда целевые значения сильно различаются (например, 30 и 110),
    модели сложнее оптимизировать оба выхода одновременно.
    Веса пытаются "угодить" обоим целям, что может приводить к колебаниям.
    
- Целевые значения, близкие к входным * весам, обучаются быстрее
- Слишком большие target values могут привести к взрывному росту градиентов
- Слишком маленькие learning rate замедляют обучение
- Идеальные target values находятся в том же масштабе, что и входные данные
"""

import numpy as np
# Функция нейронной сети (простая линейная модель)
def neural_networks(inp, weights):
    # Поэлементное умножение входного вектора на весовую матрицу
    # Это простая модель без суммирования (поэлементное умножение)
    return inp * weights

# Функция вычисления ошибки
def get_error(true_prediction, prediction):
    # Вычисление среднеквадратичной ошибки (MSE) для каждого элемента
    # Возвращает массив квадратов разностей между целевыми и предсказанными значениями
    return (true_prediction - prediction) ** 2

# Функция градиентного спуска для обучения модели
def gradient(inp, weights, true_predictions, count_iters, learning_rate):
    # Цикл обучения на заданное количество итераций
    for i in range(count_iters):
        # Прямой проход: получаем предсказание от нейронной сети
        prediction = neural_networks(inp, weights)
        
        # Вычисляем ошибку между предсказанием и целевыми значениями
        error = get_error(true_predictions, prediction)
        
        # Выводим отладочную информацию на каждой итерации
        print("Prediction: ", prediction)  # Текущее предсказание модели
        print("Weights: ", weights)        # Текущие веса модели
        print("Error: ", error)            # Текущая ошибка
        
        # Вычисление градиента (вектора поправок к весам)
        # Формула: (предсказание - цель) * вход * скорость_обучения
        delta = (prediction - true_predictions) * inp * learning_rate
        
        # Обновление весов: градиентный спуск (вычитаем градиент)
        weights = weights - delta

# Определение входных данных 
inp = np.array([150, 40])

# Инициализация весовой матрицы
# Исходная матрица 2x2, затем транспонируем для правильной размерности
weights = np.array(
    [[0.2, 0.3], [0.5, 0.7]]
).T  # Транспонируем данную весовую матрицу векторов

# Установка гиперпараметров обучения
learning_rate = 0.00001  # Скорость обучения (шаг градиентного спуска)
count_iters = 50         # Количество итераций обучения

# ЭКСПЕРИМЕНТ 1: Очень большие целевые значения
true_predictions = np.array([1000, 2000])  # Целевые значения сильно отличаются от входных
# Запуск обучения с этими целевыми значениями
gradient(inp, weights, true_predictions, count_iters, learning_rate)

print()  # Пустая строка для разделения выводов

# ЭКСПЕРИМЕНТ 2: Умеренные целевые значения 
true_predictions = np.array([70, 90])  # Более реалистичные значения
# gradient(inp, weights, true_predictions, count_iters, learning_rate)  

# ЭКСПЕРИМЕНТ 3: Разнонаправленные целевые значения
true_predictions = np.array([30, 110])  # Одно значение меньше входа, другое больше
# gradient(inp, weights, true_predictions, count_iters, learning_rate)  

# ЭКСПЕРИМЕНТ 4: Очень близкие целевые значения
true_predictions = np.array([100, 101])  # Значения очень близкие друг к другу
# gradient(inp, weights, true_predictions, count_iters, learning_rate)  
     
