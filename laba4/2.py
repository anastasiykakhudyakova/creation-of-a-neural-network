#Градиентный спуск с несколькими входами
#Если мы сравним код предыдущей работы и текущий, то увидим, что процесс обучения практически одинаковый
#Разница состоит лишь в том, то количество подбираемых весов теперь два и они представляют собой вектор.
#Проведем эксперимент и зафиксируем первый вес масива весов, подберем количество эпох достаточное для обучения.
#Мы видим, что несмотря на неизменный один из весовых коэффициентов наша нейронная сеть все же обучилась. 1
#Объективно, если весовой коэффициент есть, значит он нужен. Просто так получилось, что сеть обучилась без него. 
#Но если мы напрмер возьмем другие данные прогноза, то может возникнуть ошибка обучения. Как быть с такой проблемой поговорим вдальнейшем

#Чем меньше скорость обучения, тем больше требуется эпох для обучения нейросети
#При слишком большой скорости нейросеть может не обучиться

"""задание"""
#Напишите по памяти код из урока "Градиентный спуск с несколькими входами".
import numpy as np

# Функция нейронной сети с несколькими входами
# Принимает входной вектор и веса, возвращает скалярное произведение
def neural_networks(inp, weights):
    return inp.dot(weights)  # Вычисляем взвешенную сумму входов


# Функция для вычисления ошибки
# Использует среднеквадратичную ошибку (MSE) для одного примера
def get_error(true_prediction, prediction):
    return (true_prediction - prediction) ** 2  # Квадрат разности между целевым и предсказанным значением


# Функция градиентного спуска
def gradient(inp, weights, true_prediction, count_iters, learning_rate):
    # Цикл обучения на заданное количество итераций
    for i in range(count_iters):
        # Шаг 1: Прямое распространение 
        prediction = neural_networks(inp, weights)  # Получаем предсказание
        
        # Шаг 2: Вычисление ошибки
        error = get_error(true_prediction, prediction)  # Рассчитываем текущую ошибку
        
        # Шаг 3: Вывод отладочной информации
        print(
            "Prediction: %.10f, Weights: %s, Error: %.20f" % (prediction, weights, error)
        )
        
        # Шаг 4: Вычисление градиента и обновление весов
        delta = (prediction - true_prediction) * inp * learning_rate
        
        # Шаг 5: Обнуление градиента для первого входа (возможно, для демонстрации)
        delta[0] = 0  # Это нестандартный шаг, обычно так не делают
        
        # Шаг 6: Обновление весов (градиентный спуск)
        weights = weights - delta  # Двигаемся в направлении, противоположном градиенту


# Инициализация входных данных
inp = np.array([150, 40])  # Вектор с двумя признаками (например, рост и возраст)

# Инициализация весов (начальные значения)
weights = np.array([0.2, 0.3])  # Два веса для двух входов

# Целевое значение (то, что мы хотим предсказать)
true_prediction = 1

# Скорость обучения (learning rate) - параметр, определяющий размер шага обновления
learning_rate = 0.00001  # Малое значение для медленного, устойчивого обучения




"""задание"""
#Измените скорость обучения (learning_rate).
#Попробуйте более высокие или более низкие значения (например, 0.0001 или 0.001).
#Как это влияет на скорость сходимости и точность предсказания?
import numpy as np
# Функция нейронной сети с несколькими входами
def neural_networks(inp, weights):
    return inp.dot(weights)  # Скалярное произведение входов на веса


# Функция для вычисления ошибки (среднеквадратичная ошибка для одного примера)
def get_error(true_prediction, prediction):
    return (true_prediction - prediction) ** 2  # Квадрат разности между целевым и предсказанным значением


# Основная функция градиентного спуска
def gradient(inp, weights, true_prediction, count_iters, learning_rate):
    # Цикл обучения на заданное количество итераций
    for i in range(count_iters):
        # Прямое распространение - получение предсказания
        prediction = neural_networks(inp, weights)
        
        # Вычисление ошибки предсказания
        error = get_error(true_prediction, prediction)
        
        # Вывод отладочной информации на каждой итерации
        print(
            "Prediction: %.10f, Weights: %s, Error: %.20f"
            % (prediction, weights, error)
        )
        
        # Вычисление градиента: производная ошибки по весам
        # Формула: (prediction - true_prediction) * входы * скорость обучения
        delta = (prediction - true_prediction) * inp * learning_rate
        
        # Обнуление обновления для первого веса (нестандартная практика)
        delta[0] = 0
        
        # Обновление весов: градиентный спуск
        weights = weights - delta  # Двигаемся против направления градиента


# Входные данные: вектор с двумя признаками (например, рост=150 и возраст=40)
inp = np.array([150, 40])

# Начальные веса нейронной сети
weights = np.array([0.2, 0.3])

# Целевое значение (желаемый результат)
true_prediction = 1

# Количество итераций обучения для каждого эксперимента
count_iters = 30

# Эксперимент 1: очень маленькая скорость обучения
print("1")  # Вывод номера эксперимента
learning_rate = 0.00001  # Очень маленькая скорость обучения 
gradient(inp, weights, true_prediction, count_iters, learning_rate)

# Эксперимент 2: маленькая скорость обучения
learning_rate = 0.0001  # Маленькая скорость обучения 
print("2")  # Вывод номера эксперимента
gradient(inp, weights, true_prediction, count_iters, learning_rate)

# Эксперимент 3: средняя скорость обучения
learning_rate = 0.001  # Средняя скорость обучения 
print("3")  # Вывод номера эксперимента
gradient(inp, weights, true_prediction, count_iters, learning_rate)

# Эксперимент 4: большая скорость обучения
learning_rate = 0.01  # Большая скорость обучения 
print("4")  # Вывод номера эксперимента
gradient(inp, weights, true_prediction, count_iters, learning_rate)  # обучение ломается

# Запуск процесса обучения
# 300 итераций градиентного спуска
gradient(inp, weights, true_prediction, 300, learning_rate)



"""задание"""
#Увеличьте количество итераций цикла (например, до 100).
#Как это влияет на точность предсказаний?
#Сколько итераций необходимо для достижения близкой к идеальной точности предсказаний?
# Импорт библиотеки NumPy для работы с массивами и математическими операциями
import numpy as np
# Функция нейронной сети: вычисляет выходное значение на основе входов и весов
def neural_networks(inp, weights):
    return inp.dot(weights)  # Скалярное произведение вектора входов на вектор весов

# Функция для вычисления ошибки: использует квадратичную функцию потерь
def get_error(true_prediction, prediction):
    return (true_prediction - prediction) ** 2  # Квадрат разности между целевым и предсказанным значением

# Функция градиентного спуска: основной алгоритм обучения
def gradient(inp, weights, true_prediction, count_iters, learning_rate):
    # Цикл обучения на заданное количество итераций
    for i in range(count_iters):
        # Шаг 1: Прямое распространение (forward propagation)
        prediction = neural_networks(inp, weights)  # Получаем предсказание модели
        
        # Шаг 2: Вычисление ошибки
        error = get_error(true_prediction, prediction)  # Оцениваем качество предсказания
        
        # Шаг 3: Вывод отладочной информации (можно закомментировать для большого числа итераций)
        print(
            "Prediction: %.10f, Weights: %s, Error: %.20f" % (prediction, weights, error)  # Форматированный вывод с высокой точностью
        )
        
        # Шаг 4: Вычисление градиента (производной ошибки по весам)
        delta = (prediction - true_prediction) * inp * learning_rate
        
        # Шаг 5: Обнуление обновления для первого веса (нестандартный шаг для демонстрации)
        delta[0] = 0  # Это означает, что первый вес никогда не обновляется
        
        # Шаг 6: Обновление весов методом градиентного спуска
        weights = weights - delta  # Двигаемся в направлении, противоположном градиенту


# Определение входных данных: вектор из двух признаков
inp = np.array([150, 40])  # Например: рост = 150, возраст = 40

# Начальные значения весов нейронной сети (случайная инициализация)
weights = np.array([0.2, 0.3])  # Два весовых коэффициента

# Целевое значение (идеальное предсказание, к которому стремимся)
true_prediction = 1

# Начальное количество итераций (будет переопределено ниже)
count_iters = 30  # Это значение не используется в эксперименте

# Эксперимент с большим количеством итераций
print("1")  # Метка для идентификации эксперимента

# Установка очень маленькой скорости обучения
learning_rate = 0.00001  # 10^-5 - очень маленький шаг обновления

# Увеличение количества итераций до 1000
count_iters = 1000  # Значительно больше итераций для компенсации малой скорости обучения

# Запуск процесса градиентного спуска
gradient(inp, weights, true_prediction, count_iters, learning_rate)

"""
Анализ влияния количества итераций на точность предсказания:

1. КАК ЭТО ВЛИЯЕТ НА ТОЧНОСТЬ ПРЕДСКАЗАНИЯ?
   - При таком большом количестве итераций (1000) нейросеть смогла обучиться.
   - Даже учитывая такую маленькую скорость обучения (0.00001), модель постепенно приближается к оптимуму.
   - Большее количество итераций позволяет совершить больше шагов градиентного спуска, 
     что особенно важно при малой скорости обучения.

2. КОГДА НЕЙРОСЕТЬ ПЕРЕСТАЕТ УЛУЧШАТЬ ПРЕДСКАЗАНИЕ?
   - Когда ошибка становится равной 0 (или практически 0).
   - Когда градиент становится близким к нулю (веса почти не изменяются).
   - Если скорость обучения слишком большая - может возникнуть расходимость или колебания.

3. СКОЛЬКО ИТЕРАЦИЙ НЕОБХОДИМО ДЛЯ ДОСТИЖЕНИЯ БЛИЗКОЙ К ИДЕАЛЬНОЙ ТОЧНОСТИ?
   Это зависит от нескольких факторов:
   а) Скорости обучения (learning_rate)
   б) Сложности задачи
   в) Начальных значений весов
   г) Желаемого уровня точности
   
   В данном примере с learning_rate = 0.00001:
   - Первые 100 итераций: медленное улучшение
   - 100-500 итераций: заметное уменьшение ошибки
   - 500-1000 итераций: постепенное приближение к оптимуму
   
   Для достижения практически идеальной точности (ошибка < 0.0001) может потребоваться:
   - При learning_rate = 0.00001: более 5000 итераций
   - При learning_rate = 0.001: около 100-200 итераций
   - При learning_rate = 0.01: может расходиться из-за слишком большого шага

