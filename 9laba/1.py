"""Улучшение распознавания рукописных цифр и пакетный градиентный спуск"""
""" задание
Добавьте dropout в код по распознаванию рукописных цифр
"""

# Импорт необходимых библиотек
import numpy as np  # Библиотека для научных вычислений и работы с массивами
from tensorflow.keras.datasets import mnist  # Загрузка набора данных MNIST

# Функция активации ReLU (Rectified Linear Unit)
def relu(x):
    return (x > 0) * x  # Возвращает x если x > 0, иначе 0

# Производная функции ReLU
def reluderiv(x):
    return (x > 0)  # Возвращает 1 если x > 0, иначе 0

# Параметры обучения нейронной сети
train_images_count = 1000  # Количество изображений для обучения (уменьшено для ускорения)
test_images_count = 10000  # Количество изображений для тестирования
pixels_per_image = 28 * 28  # Размер одного изображения (28x28 пикселей = 784)
digits_num = 10  # Количество классов (цифры от 0 до 9)

# Загрузка и разделение набора данных MNIST
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Подготовка тренировочных данных
# Изменение формы из 3D (1000, 28, 28) в 2D (1000, 784) и нормализация [0, 255] -> [0, 1]
train_images = x_train[0:train_images_count].reshape(train_images_count, pixels_per_image) / 255
train_labels_original = y_train[0:train_images_count]  # Оригинальные метки (0-9)

# Подготовка тестовых данных
test_images = x_test[0:test_images_count].reshape(test_images_count, pixels_per_image) / 255
test_labels_original = y_test[0:test_images_count]  # Оригинальные метки (0-9)

# One-Hot Encoding для тренировочных меток
# Преобразование меток в бинарный вектор (например, 3 -> [0,0,0,1,0,0,0,0,0,0])
train_labels = np.zeros((len(train_labels_original), digits_num))  # Создание матрицы нулей
for j in range(len(train_labels_original)):
    train_labels[j][train_labels_original[j]] = 1  # Установка 1 на позиции соответствующей цифре

# One-Hot Encoding для тестовых меток
test_labels = np.zeros((len(test_labels_original), digits_num))
for i, j in enumerate(test_labels_original):
    test_labels[i][j] = 1

# Инициализация весов случайными значениями
np.random.seed(2)  # Фиксация случайных чисел для воспроизводимости
hidden_size = 50  # Количество нейронов в скрытом слое

# Веса между входным (784) и скрытым (50) слоем
weight_hid = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1
# Веса между скрытым (50) и выходным (10) слоем
weight_out = 0.2 * np.random.random((hidden_size, digits_num)) - 0.1

# Параметры обучения
learning_rate = 0.01  # Скорость обучения (шаг градиентного спуска)
num_epoch = 100  # Количество полных проходов через весь набор данны

"""Пакетный градиентный спуск"""
#Ранее пройден материал по стохастическому градиентному спуску при обучении на наборе данных, когда весовые коэффициенты изменяются с каждым набором данных и на каждой итерации. 
#Также рассмотрели обучение по алгоритму полного градиентного спуска, когда весовые коэффициенты изменяются только после прохождения всего набора даных как среднее арифметичечское значений дельты и количества наборов данных.
#Сегодня мы рассмотрим пакетный градиентный спуск, преобразовав код с прошлого занятия. Сохраним полученную точность и начнем изменять код.
#после объявления количества эпох создадим переменную, которая хранит количество данных в одном пакете
batch_size = 50  # Размер пакета для пакетного градиентного спуска
# Обучение нейронной сети с Dropout и пакетным градиентным спуском
print("Начало обучения с Dropout и пакетным градиентным спуском...")
print(f"Размер пакета: {batch_size}")
print(f"Количество итераций в эпохе: {int(len(train_images)/batch_size)}")


# Цикл обучения по эпохам
for i in range(num_epoch):
    correct_answers = 0# Счетчик правильных ответов на тренировочных данных
    #обучение будет проходить не по всем данным, а только по данным, помещенным в пакет. Т.е. мы получим 20 итераций по 50 тренировочных данных в пакете
    # Пакетный градиентный спуск: обрабатываем данные не по одному, а пакетами
    for j in range(int(len(train_images) / batch_size)):  #определеяем индексы будущих массивов, чтобы понимать с какой порцией данных работаем
        # Определяем границы текущего пакета
        batch_start = batch_size * j  # Начальный индекс пакета
        batch_end = batch_size * (j + 1)  # Конечный индекс пакета
        #передаем новые индексы для определения прогноза входного слоя. Берем изображения от 0 до 49 включительно
        # Прямое распространение (forward propagation) для пакета
        layer_in = train_images[batch_start:batch_end]  # Входной слой: batch_size изображений  # Матрица из batch_size изображений #Это уже не просто последовательность векторов, а целая матрица из 50 изображений
        # Скрытый слой: активация ReLU от произведения входов и весов
        layer_hid = relu(np.dot(layer_in, weight_hid))#Получаем 50 конечных результатов
#Алгоритм распознавания рукописных цифр, пройденный ранее, можно значительно улучшить с помощью методики регуляризации.
#Данная методика нужна для того, чтобы предотвратить переобучение сети.
#Один из методов регуляризации это dropout (отсев). Как он работает: внутри второго цикла стохастического спуска мы практически отключаем некоторые нейроны. 
#Причем на каждой итерации мы отключаем разные блоки нейронов, задействовав при этом отключенные ранее.
#Т.е. мы уже не пытаемся обучать находить точные совпадения, а лишь некоторые подобия (очертания). Сеть учится не на всех данных целиком, а только на части данных.
#Напишем dropout-маску для ввода в наш алгоритм регуляризации по типу отсева некоторых данных для обучения. Данную маску будем применять для регулировки прогноза на скрытом слое.

#После вычисления layer_hid в код предыдущего занятия добавляем следующие строки
#создадим dropout маску
        # Добавляем Dropout регуляризацию (начинаем применять после первой эпохи)
        if i > 0:# Начинаем применять Dropout после первой эпохи
            # Создание маски Dropout: случайные 0 и 1 с вероятностью 50%
            dropout_mask = np.random.randint(2, size=layer_hid.shape)# Маска из 0 и 1 # случайное число от 0 до 1 (максимальное число 2) с размером равному скрытому слою
            # Умножение на маску (отключаем часть нейронов) и на 2 для компенсации
            layer_hid *= dropout_mask * 2  # Умножаем на 2 для компенсации отключенных нейронов
        #Т.е. мы фактически перемножаем массив значений нейронов скрытого слоя на массив нашей маски. В примере ниже для обучения остается только один нейрон
        # Выходной слой: линейная комбинация значений скрытого слоя
        layer_out = np.dot(layer_hid, weight_out)  # Получаем batch_size конечных результатов

      
# Так как вычисление выполнено только для одного элемента, то соответственно определение корректности ответа необходимо выполнить в цикле из 50 обозначений. 
#В layer_out берем конкретный столбец. так как в layer_labels 1000 обозначений, а нам нужен диапазон от 1 до 50 (от 50 до 100 и тк далее), то мы прибавляем индексы текущего массива
        # Подсчет правильных ответов для всего пакета
        for k in range(batch_size):
            # Сравниваем предсказанную цифру (максимальное значение в layer_out)
            # с фактической цифрой (максимальное значение в train_labels)
            correct_answers += int(np.argmax(layer_out[k:k+1]) == 
                                  np.argmax(train_labels[batch_start + k:batch_end + k]))
         #осталось рассчитать выходную дельту изменения весовой матрицы. Для это нужно найти среднее арифметическое значение от количества элементов в пакете
        # Обратное распространение ошибки для пакета
        # Обратное распространение ошибки (backpropagation)
        # Вычисление дельты для выходного слоя
        layer_out_delta = (layer_out - train_labels[batch_start:batch_end]) / batch_size
#Но подобное изменение массива значений нейронов скрытого слоя приводит к уменьшению их количества вдвое, что приведет к проблемам при дальнейших расчетах выходного прогноза и обучения. 
#Поэтому для полной регуляризации необходимо значение прогноза скрытого слоя дополнительно умножить на 2. И учесть маску при обучении в процессе обратного распространения.
        if i > 0:# Вычисление дельты для скрытого слоя с учетом Dropout
            # Учитываем маску Dropout при обратном распространении
            layer_hid_delta = layer_out_delta.dot(weight_out.T) * reluderiv(layer_hid) * dropout_mask
        else:
            layer_hid_delta = layer_out_delta.dot(weight_out.T) * reluderiv(layer_hid)
        
        # Обновление весов с помощью градиентного спуска
        weight_out -= learning_rate * layer_hid.T.dot(layer_out_delta)
        weight_hid -= learning_rate * layer_in.T.dot(layer_hid_delta)
    
    # Вывод точности после каждой эпохи
    accuracy = correct_answers * 100 / len(train_images)
    print(f"Эпоха: {i+1}, Точность на тренировочных данных: {accuracy:.2f}%")



# Тестирование на тестовых данных (без Dropout на тестировании)
print("\nТестирование на тестовых данных...")
print("Примечание: Dropout отключен на этапе тестирования")
correct_answers = 0

# Проход по всем тестовым изображениям
for j in range(len(test_images)):
    # Прямое распространение (без Dropout - важное отличие от обучения)
    layer_in = test_images[j:j+1]  # Одно тестовое изображение
    layer_hid = relu(np.dot(layer_in, weight_hid))
    layer_out = np.dot(layer_hid, weight_out)
    
    # Подсчет правильных ответов
    correct_answers += int(np.argmax(layer_out) == np.argmax(test_labels[j:j+1]))

# Вывод итоговой точности на тестовых данных
test_accuracy = correct_answers * 100 / len(test_images)
print(f"Точность на тестовых данных: {test_accuracy:.2f}%")

# Дополнительная информация
print(f"\nПараметры обучения с регуляризацией:")
print(f"  Размер обучающей выборки: {train_images_count}")
print(f"  Размер тестовой выборки: {test_images_count}")
print(f"  Количество эпох: {num_epoch}")
print(f"  Скорость обучения: {learning_rate}")
print(f"  Количество нейронов в скрытом слое: {hidden_size}")
print(f"  Размер пакета: {batch_size}")
print(f"  Использован Dropout: Да (после первой эпохи)")
print(f"  Вероятность Dropout: 50% (mask * 2)")

# Демонстрация работы модели на первых 10 тестовых изображениях
print("\nПримеры предсказаний (первые 10 изображений тестовой выборки):")
for i in range(10):
    layer_in = test_images[i:i+1]
    layer_hid = relu(np.dot(layer_in, weight_hid))
    layer_out = np.dot(layer_hid, weight_out)
    predicted = np.argmax(layer_out)  # Предсказанная цифра
    actual = np.argmax(test_labels[i:i+1])  # Фактическая цифра
    print(f"  Изображение {i+1}: Предсказано = {predicted}, Фактически = {actual} {'✓' if predicted == actual else '✗'}")
 
#Запускаем и оцениваем увеличение скорости обучения и точности на тестовых данных. В случае получения неудовлетворительной точности можно попробовать увеличить скорость обучения или количество эпох.
#Увеличение скорости обучения связано с тем, что вычисления итеративно ведутся не с одним вектором, а сразу с пакетом веторов. В результате чего количество итераций становится в 20 раз меньше.
# Сравнение с оригинальным алгоритмом (без Dropout)
"""Оригинальный алгоритм стохастический градиентный спуск
- Плюсы: Простота реализации
- Минусы: Медленнее сходится, склонен к переобучению

Алгоритм с Dropout и пакетным градиентным спуском
- Плюсы:
 * Быстрее обучение (меньше итераций)
 * Регуляризация предотвращает переобучение
 * Лучшая обобщающая способность
 - Минусы:
 * Сложнее реализация
 * Требует настройки дополнительных параметров"""

